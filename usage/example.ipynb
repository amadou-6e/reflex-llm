{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RefLex LLM Usage Examples\n",
                "\n",
                "This notebook demonstrates how to use RefLex LLM for intelligent OpenAI API provider resolution with automatic fallback to local AI servers.\n",
                "\n",
                "## Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install reflex-llms"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Basic Setup\n",
                "\n",
                "### Environment Variables\n",
                "\n",
                "First, set up your API credentials (optional - RefLex will fallback to local AI if not available):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Environment setup complete\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "# Set up environment variables (optional)\n",
                "# os.environ['OPENAI_API_KEY'] = 'your-openai-api-key'\n",
                "# os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://your-resource.openai.azure.com/'\n",
                "# os.environ['AZURE_OPENAI_API_KEY'] = 'your-azure-api-key'\n",
                "# os.environ['AZURE_OPENAI_API_VERSION'] = '2024-02-15-preview'\n",
                "\n",
                "print(\"Environment setup complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Docker Setup (for local AI fallback)\n",
                "\n",
                "RefLex can automatically spin up local AI servers using Docker:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'Docker version 28.1.1, build 4eba377\\n'"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import subprocess\n",
                "\n",
                "subprocess.run(['docker', '--version'], capture_output=True, text=True).stdout"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quick Start Examples\n",
                "\n",
                "### 1. Basic Usage (Automatic Provider Selection)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking OpenAI API providers...\n",
                        "  Trying openai...\n",
                        "  Using OpenAI API\n",
                        "Response: Of course! I'd be happy to help you with Python. What specific questions do you have or what do you need assistance with?\n",
                        "Using provider: openai\n"
                    ]
                }
            ],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Get OpenAI client with automatic provider resolution\n",
                "client = reflex_llms.get_openai_client()\n",
                "\n",
                "# Use exactly like the OpenAI client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"Hello! Can you help me with Python?\"}\n",
                "    ],\n",
                "    max_tokens=100\n",
                ")\n",
                "\n",
                "print(f\"Response: {response.choices[0].message.content}\")\n",
                "print(f\"Using provider: {reflex_llms.get_selected_provider()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Check Which Provider is Being Used"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using cached openai configuration\n",
                        "Selected provider: openai\n",
                        "Using cloud AI service\n",
                        "Status: {'selected_provider': 'openai', 'has_cached_config': True, 'reflex_server_running': False, 'reflex_server_url': None}\n"
                    ]
                }
            ],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Check provider without making API calls\n",
                "provider_type = reflex_llms.get_openai_client_type()\n",
                "print(f\"Selected provider: {provider_type}\")\n",
                "\n",
                "# Check if using local AI\n",
                "if reflex_llms.is_using_reflex():\n",
                "    print(\"Using local RefLex server\")\n",
                "    server = reflex_llms.get_reflex_server()\n",
                "    if server:\n",
                "        print(f\"Server URL: {server.openai_compatible_url}\")\n",
                "        print(f\"Server healthy: {server.is_healthy}\")\n",
                "else:\n",
                "    print(\"Using cloud AI service\")\n",
                "\n",
                "# Get comprehensive status\n",
                "status = reflex_llms.get_module_status()\n",
                "print(f\"Status: {status}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration Examples\n",
                "\n",
                "### 3. Custom Configuration Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Custom OpenAI API URL (e.g., for enterprise deployments)\n",
                "client = reflex_llms.get_openai_client(\n",
                "    preference_order=[\"openai\"],\n",
                "    openai_base_url=\"https://enterprise-api.openai.com/v1\",\n",
                "    timeout=10.0,\n",
                ")\n",
                "\n",
                "# Custom Azure configuration\n",
                "client = reflex_llms.get_openai_client(\n",
                "    preference_order=[\"azure\", \"openai\"],\n",
                "    azure_api_version=\"2024-06-01\",\n",
                "    azure_base_url=\"https://custom-azure.openai.azure.com\",\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using cached openai configuration\n",
                        "Custom configurations set up successfully\n"
                    ]
                }
            ],
            "source": [
                "from reflex_llms.server import ReflexServerConfig, ModelMapping\n",
                "\n",
                "# Custom RefLex server configuration\n",
                "reflex_config = ReflexServerConfig(\n",
                "    port=8080,\n",
                "    container_name=\"my-ai-server\",\n",
                "    model_mappings=ModelMapping(minimal_setup=True),\n",
                ")\n",
                "\n",
                "client = reflex_llms.get_openai_client(\n",
                "    preference_order=[\"reflex\"],\n",
                "    reflex_server_config=reflex_config\n",
                ")\n",
                "\n",
                "print(\"Custom configurations set up successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "reflex_config = {\n",
                "    \"port\": 8080,\n",
                "    \"container_name\": \"my-ai-server\",\n",
                "    \"model_mappings\": {\n",
                "        \"minimal_setup\": True\n",
                "    },\n",
                "}\n",
                "client = reflex_llms.get_openai_client(\n",
                "    preference_order=[\"reflex\"],\n",
                "    reflex_server_config=reflex_config\n",
                ")\n",
                "\n",
                "print(\"Custom configurations set up successfully\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuration File Examples\n",
                "\n",
                "### 5. Using reflex.json Configuration Files\n",
                "\n",
                "Create a `reflex.json` file in your project directory:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'usage'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01musage\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display_json\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'usage'"
                    ]
                }
            ],
            "source": [
                "from usage.utils import display_json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import reflex_llms\n",
                "\n",
                "print(\"Created reflex.json configuration file\")\n",
                "\n",
                "# Use the configuration\n",
                "client = reflex_llms.get_openai_client(from_file=True)\n",
                "print(f\"Using provider from file: {reflex_llms.get_selected_provider()}\")\n",
                "\n",
                "# Override specific settings while using file\n",
                "reflex_llms.clear_cache()\n",
                "client = reflex_llms.get_openai_client(\n",
                "    from_file=True,\n",
                "    timeout=15.0,  # Override file setting\n",
                "    preference_order=[\"reflex\", \"openai\"]  # Override file setting\n",
                ")\n",
                "print(f\"Using provider with overrides: {reflex_llms.get_selected_provider()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6. Creating Example Configuration Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Create an example reflex.json with all options\n",
                "reflex_llms.create_example_config(\"my-reflex-config.json\")\n",
                "\n",
                "# Check if a config file is being used\n",
                "config_path = reflex_llms.get_reflex_config_path()\n",
                "if config_path:\n",
                "    print(f\"Using config file: {config_path}\")\n",
                "else:\n",
                "    print(\"No config file found, using defaults\")\n",
                "\n",
                "# Initialize with config discovery (like load_dotenv)\n",
                "reflex_llms.init_reflex()\n",
                "print(\"RefLex initialization complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Practical Usage Examples\n",
                "\n",
                "### 7. Chat Application Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "def chat_with_ai(user_input=\"Hello, how are you?\"):\n",
                "    \"\"\"Simple chat function that works with any provider.\"\"\"\n",
                "    client = reflex_llms.get_openai_client()\n",
                "    \n",
                "    print(f\"Chat ready! Using {reflex_llms.get_selected_provider()}\")\n",
                "    \n",
                "    conversation = [{\"role\": \"user\", \"content\": user_input}]\n",
                "    \n",
                "    try:\n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=conversation,\n",
                "            max_tokens=150\n",
                "        )\n",
                "        \n",
                "        ai_response = response.choices[0].message.content\n",
                "        print(f\"You: {user_input}\")\n",
                "        print(f\"AI: {ai_response}\")\n",
                "        \n",
                "        return ai_response\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        print(\"Trying to reconnect...\")\n",
                "        \n",
                "        # Clear cache and try again\n",
                "        reflex_llms.clear_cache()\n",
                "        client = reflex_llms.get_openai_client()\n",
                "        return \"Sorry, I'm having trouble connecting right now.\"\n",
                "\n",
                "# Run the chat example\n",
                "response = chat_with_ai(\"What is artificial intelligence?\")\n",
                "print(f\"\\nResponse received: {len(response)} characters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 8. Document Analysis Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "def analyze_document(text, analysis_type=\"summary\"):\n",
                "    \"\"\"Analyze a document using available AI provider.\"\"\"\n",
                "    client = reflex_llms.get_openai_client()\n",
                "    \n",
                "    prompts = {\n",
                "        \"summary\": \"Please provide a concise summary of the following text:\",\n",
                "        \"sentiment\": \"Analyze the sentiment of the following text:\",\n",
                "        \"keywords\": \"Extract the main keywords from the following text:\"\n",
                "    }\n",
                "    \n",
                "    prompt = prompts.get(analysis_type, prompts[\"summary\"])\n",
                "    \n",
                "    response = client.chat.completions.create(\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        messages=[\n",
                "            {\"role\": \"system\", \"content\": \"You are a helpful document analysis assistant.\"},\n",
                "            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\n{text}\"}\n",
                "        ],\n",
                "        max_tokens=200\n",
                "    )\n",
                "    \n",
                "    return {\n",
                "        \"analysis\": response.choices[0].message.content,\n",
                "        \"provider\": reflex_llms.get_selected_provider(),\n",
                "        \"model\": \"gpt-3.5-turbo\"\n",
                "    }\n",
                "\n",
                "# Example usage\n",
                "sample_text = \"\"\"\n",
                "RefLex LLM is a Python library that provides intelligent OpenAI API provider \n",
                "resolution with automatic fallback capabilities. It seamlessly switches between \n",
                "OpenAI, Azure OpenAI, and local Ollama-based servers based on availability and \n",
                "user preferences. This ensures your applications remain functional even when \n",
                "cloud services are unavailable.\n",
                "\"\"\"\n",
                "\n",
                "result = analyze_document(sample_text, \"summary\")\n",
                "print(f\"Analysis: {result['analysis']}\")\n",
                "print(f\"Provider: {result['provider']}\")\n",
                "\n",
                "# Try sentiment analysis\n",
                "sentiment_result = analyze_document(sample_text, \"sentiment\")\n",
                "print(f\"\\nSentiment: {sentiment_result['analysis']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 9. Embedding Generation Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "import numpy as np\n",
                "\n",
                "def get_embeddings(texts):\n",
                "    \"\"\"Generate embeddings using available provider.\"\"\"\n",
                "    client = reflex_llms.get_openai_client()\n",
                "    \n",
                "    # Handle both single text and list of texts\n",
                "    if isinstance(texts, str):\n",
                "        texts = [texts]\n",
                "    \n",
                "    embeddings = []\n",
                "    \n",
                "    for text in texts:\n",
                "        response = client.embeddings.create(\n",
                "            model=\"text-embedding-ada-002\",\n",
                "            input=text\n",
                "        )\n",
                "        embeddings.append(response.data[0].embedding)\n",
                "    \n",
                "    return np.array(embeddings)\n",
                "\n",
                "# Example usage\n",
                "sample_texts = [\n",
                "    \"RefLex provides AI fallback capabilities\",\n",
                "    \"OpenAI API integration with local alternatives\",\n",
                "    \"Docker-based local AI server management\"\n",
                "]\n",
                "\n",
                "try:\n",
                "    embeddings = get_embeddings(sample_texts)\n",
                "    print(f\"Generated {len(embeddings)} embeddings using {reflex_llms.get_selected_provider()}\")\n",
                "    print(f\"Embedding shape: {embeddings.shape}\")\n",
                "    \n",
                "    # Calculate similarity between first two texts\n",
                "    similarity = np.dot(embeddings[0], embeddings[1])\n",
                "    print(f\"Similarity between first two texts: {similarity:.3f}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Embedding generation failed: {e}\")\n",
                "    print(\"This might happen if the provider doesn't support embeddings\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advanced Configuration Examples\n",
                "\n",
                "### 10. Production Deployment Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "from reflex_llms.server import ReflexServerConfig, ModelMapping\n",
                "import os\n",
                "\n",
                "def setup_production_ai():\n",
                "    \"\"\"Production-ready AI setup with fallback.\"\"\"\n",
                "    \n",
                "    # Production configuration\n",
                "    prod_config = ReflexServerConfig(\n",
                "        host=\"0.0.0.0\",  # Accept connections from all interfaces\n",
                "        port=8080,\n",
                "        container_name=\"production-ai-server\",\n",
                "        data_path=\"/opt/ai-models\",  # Persistent storage\n",
                "        model_mappings=ModelMapping(\n",
                "            minimal_setup=False,  # Full model set for production\n",
                "            model_mapping={\n",
                "                \"gpt-3.5-turbo\": \"llama3.2:7b\",  # Larger models for better quality\n",
                "                \"gpt-4\": \"llama3.1:70b\",\n",
                "                \"gpt-4o\": \"gemma3:27b\",\n",
                "                \"text-embedding-ada-002\": \"nomic-embed-text\"\n",
                "            }\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    # Try cloud first, fallback to local\n",
                "    client = reflex_llms.get_openai_client(\n",
                "        preference_order=[\"openai\", \"azure\", \"reflex\"],\n",
                "        timeout=30.0,  # Longer timeout for production\n",
                "        reflex_server_config=prod_config\n",
                "    )\n",
                "    \n",
                "    provider = reflex_llms.get_selected_provider()\n",
                "    print(f\"Production AI ready using: {provider}\")\n",
                "    \n",
                "    if provider == \"reflex\":\n",
                "        server = reflex_llms.get_reflex_server()\n",
                "        if server:\n",
                "            print(f\"Local server status: {server.get_status()}\")\n",
                "    \n",
                "    return client\n",
                "\n",
                "# Setup production environment\n",
                "try:\n",
                "    prod_client = setup_production_ai()\n",
                "    print(\"Production setup complete\")\n",
                "except Exception as e:\n",
                "    print(f\"Production setup failed: {e}\")\n",
                "    print(\"This is normal if Docker is not available\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 11. Development Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "from reflex_llms.server import ReflexServerConfig, ModelMapping\n",
                "\n",
                "def setup_development_ai():\n",
                "    \"\"\"Fast development setup with minimal models.\"\"\"\n",
                "    \n",
                "    # Development configuration - fast startup\n",
                "    dev_config = ReflexServerConfig(\n",
                "        port=11434,\n",
                "        container_name=\"dev-ai-server\",\n",
                "        model_mappings=ModelMapping(\n",
                "            minimal_setup=True,  # Only essential models\n",
                "            minimal_model_mapping={\n",
                "                \"gpt-3.5-turbo\": \"llama3.2:3b\",  # Small, fast model\n",
                "                \"gpt-4o-mini\": \"gemma3:2b\",      # Tiny model for testing\n",
                "                \"text-embedding-ada-002\": \"nomic-embed-text\"\n",
                "            }\n",
                "        )\n",
                "    )\n",
                "    \n",
                "    # Prefer local for development (no API costs)\n",
                "    client = reflex_llms.get_client_dev_mode(\n",
                "        reflex_server_config=dev_config,\n",
                "        timeout=10.0\n",
                "    )\n",
                "    \n",
                "    print(f\"Development AI ready using: {reflex_llms.get_selected_provider()}\")\n",
                "    return client\n",
                "\n",
                "# Setup development environment\n",
                "try:\n",
                "    dev_client = setup_development_ai()\n",
                "    print(\"Development setup complete\")\n",
                "except Exception as e:\n",
                "    print(f\"Development setup failed: {e}\")\n",
                "    print(\"Falling back to available providers\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Error Handling and Monitoring\n",
                "\n",
                "### 12. Robust Error Handling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "import time\n",
                "\n",
                "def robust_ai_call(messages, max_retries=3):\n",
                "    \"\"\"Make AI calls with automatic retry and provider switching.\"\"\"\n",
                "    \n",
                "    for attempt in range(max_retries):\n",
                "        try:\n",
                "            client = reflex_llms.get_openai_client()\n",
                "            \n",
                "            response = client.chat.completions.create(\n",
                "                model=\"gpt-3.5-turbo\",\n",
                "                messages=messages,\n",
                "                max_tokens=100\n",
                "            )\n",
                "            \n",
                "            return {\n",
                "                \"success\": True,\n",
                "                \"response\": response.choices[0].message.content,\n",
                "                \"provider\": reflex_llms.get_selected_provider(),\n",
                "                \"attempt\": attempt + 1\n",
                "            }\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
                "            \n",
                "            if attempt < max_retries - 1:\n",
                "                # Clear cache and try different provider\n",
                "                reflex_llms.clear_cache()\n",
                "                time.sleep(2 ** attempt)  # Exponential backoff\n",
                "            else:\n",
                "                return {\n",
                "                    \"success\": False,\n",
                "                    \"error\": str(e),\n",
                "                    \"attempts\": max_retries\n",
                "                }\n",
                "\n",
                "# Example usage\n",
                "messages = [{\"role\": \"user\", \"content\": \"What is machine learning?\"}]\n",
                "result = robust_ai_call(messages)\n",
                "\n",
                "if result[\"success\"]:\n",
                "    print(f\"Success with {result['provider']}: {result['response'][:100]}...\")\n",
                "else:\n",
                "    print(f\"All attempts failed: {result['error']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 13. Health Monitoring"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "import time\n",
                "\n",
                "def monitor_ai_health():\n",
                "    \"\"\"Monitor AI provider health and performance.\"\"\"\n",
                "    \n",
                "    status = reflex_llms.get_module_status()\n",
                "    print(\"=== AI Health Report ===\")\n",
                "    print(f\"Selected Provider: {status['selected_provider']}\")\n",
                "    print(f\"Config Cached: {status['has_cached_config']}\")\n",
                "    \n",
                "    if status['reflex_server_running']:\n",
                "        print(f\"RefLex Server: Running at {status['reflex_server_url']}\")\n",
                "        \n",
                "        server = reflex_llms.get_reflex_server()\n",
                "        if server:\n",
                "            try:\n",
                "                server_status = server.get_status()\n",
                "                print(f\"  Total Models: {server_status.get('total_models', 'Unknown')}\")\n",
                "                print(f\"  OpenAI Models: {len(server_status.get('openai_compatible_models', []))}\")\n",
                "                print(f\"  Container Running: {server_status.get('container_running', False)}\")\n",
                "print(f\"  Port Open: {server_status.get('port_open', False)}\")\n",
                "            except Exception as e:\n",
                "                print(f\"  Error getting server status: {e}\")\n",
                "    else:\n",
                "        print(\"RefLex Server: Not running\")\n",
                "    \n",
                "    # Test API call performance\n",
                "    try:\n",
                "        start_time = time.time()\n",
                "        client = reflex_llms.get_openai_client()\n",
                "        \n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
                "            max_tokens=5\n",
                "        )\n",
                "        \n",
                "        response_time = time.time() - start_time\n",
                "        print(f\"API Response Time: {response_time:.2f}s\")\n",
                "        print(f\"API Status: Healthy\")\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"API Status: Error - {e}\")\n",
                "\n",
                "# Run health check\n",
                "monitor_ai_health()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cache Management\n",
                "\n",
                "### 14. Cache Control"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Check current cache status\n",
                "status = reflex_llms.get_module_status()\n",
                "print(f\"Cache status: {status['has_cached_config']}\")\n",
                "print(f\"Current provider: {status['selected_provider']}\")\n",
                "\n",
                "# Force provider re-evaluation\n",
                "print(\"\\nForcing re-evaluation...\")\n",
                "config = reflex_llms.get_openai_client_config(force_recheck=True)\n",
                "print(f\"Re-evaluated provider: {reflex_llms.get_selected_provider()}\")\n",
                "\n",
                "# Clear cache manually\n",
                "print(\"\\nClearing cache...\")\n",
                "reflex_llms.clear_cache()\n",
                "print(f\"Cache cleared: {reflex_llms.get_module_status()['has_cached_config']}\")\n",
                "\n",
                "# Stop RefLex server if running\n",
                "if reflex_llms.is_using_reflex():\n",
                "    print(\"Stopping RefLex server...\")\n",
                "    reflex_llms.stop_reflex_server()\n",
                "    print(\"RefLex server stopped\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Best Practices\n",
                "\n",
                "### 15. Recommended Usage Patterns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "import os\n",
                "\n",
                "class AIManager:\n",
                "    \"\"\"Best practice AI manager class.\"\"\"\n",
                "    \n",
                "    def __init__(self, environment=\"development\"):\n",
                "        self.environment = environment\n",
                "        self.client = None\n",
                "        self._setup_client()\n",
                "    \n",
                "    def _setup_client(self):\n",
                "        \"\"\"Setup client based on environment.\"\"\"\n",
                "        if self.environment == \"development\":\n",
                "            # Fast startup, prefer local\n",
                "            self.client = reflex_llms.get_client_dev_mode(\n",
                "                timeout=5.0,\n",
                "                from_file=True  # Load from reflex.json if available\n",
                "            )\n",
                "        elif self.environment == \"production\":\n",
                "            # Reliable, prefer cloud\n",
                "            self.client = reflex_llms.get_client_prod_mode(\n",
                "                timeout=30.0,\n",
                "                from_file=True\n",
                "            )\n",
                "        else:\n",
                "            # Default setup\n",
                "            self.client = reflex_llms.get_openai_client(from_file=True)\n",
                "        \n",
                "        print(f\"AI Manager ready using: {reflex_llms.get_selected_provider()}\")\n",
                "    \n",
                "    def chat_completion(self, messages, **kwargs):\n",
                "        \"\"\"Robust chat completion with retry.\"\"\"\n",
                "        try:\n",
                "            return self.client.chat.completions.create(\n",
                "                messages=messages,\n",
                "                **kwargs\n",
                "            )\n",
                "        except Exception as e:\n",
                "            print(f\"AI request failed: {e}\")\n",
                "            # Retry with fresh client\n",
                "            reflex_llms.clear_cache()\n",
                "            self._setup_client()\n",
                "            return self.client.chat.completions.create(\n",
                "                messages=messages,\n",
                "                **kwargs\n",
                "            )\n",
                "    \n",
                "    def get_embeddings(self, text):\n",
                "        \"\"\"Generate embeddings with error handling.\"\"\"\n",
                "        try:\n",
                "            return self.client.embeddings.create(\n",
                "                model=\"text-embedding-ada-002\",\n",
                "                input=text\n",
                "            )\n",
                "        except Exception as e:\n",
                "            print(f\"Embedding request failed: {e}\")\n",
                "            raise\n",
                "    \n",
                "    def health_check(self):\n",
                "        \"\"\"Check AI system health.\"\"\"\n",
                "        return reflex_llms.get_module_status()\n",
                "    \n",
                "    def cleanup(self):\n",
                "        \"\"\"Clean shutdown.\"\"\"\n",
                "        if reflex_llms.is_using_reflex():\n",
                "            reflex_llms.stop_reflex_server()\n",
                "\n",
                "# Usage examples\n",
                "ai_dev = AIManager(\"development\")\n",
                "ai_prod = AIManager(\"production\")\n",
                "\n",
                "# Use the managers\n",
                "try:\n",
                "    response = ai_dev.chat_completion([\n",
                "        {\"role\": \"user\", \"content\": \"Hello from development!\"}\n",
                "    ], model=\"gpt-3.5-turbo\", max_tokens=20)\n",
                "    \n",
                "    print(f\"Dev response: {response.choices[0].message.content}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Development AI failed: {e}\")\n",
                "\n",
                "# Health check\n",
                "health = ai_dev.health_check()\n",
                "print(f\"\\nAI Health: {health['selected_provider']} - {health.get('reflex_server_running', False)}\")\n",
                "\n",
                "# Cleanup\n",
                "ai_dev.cleanup()\n",
                "ai_prod.cleanup()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "RefLex LLM provides:\n",
                "\n",
                "1. **Seamless API Compatibility** - Drop-in replacement for OpenAI clients\n",
                "2. **Intelligent Fallback** - Automatic switching between providers\n",
                "3. **Flexible Configuration** - File-based, environment, or programmatic setup\n",
                "4. **Production Ready** - Caching, error handling, and monitoring\n",
                "5. **Development Friendly** - Fast local AI setup for development\n",
                "\n",
                "The library ensures your AI applications remain functional regardless of cloud service availability, making them more resilient and cost-effective."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. **Install RefLex LLM**: `pip install reflex-llm`\n",
                "2. **Set up your environment**: Add API keys or ensure Docker is available\n",
                "3. **Create a `reflex.json`**: Configure your preferred settings\n",
                "4. **Start building**: Use RefLex as a drop-in OpenAI replacement\n",
                "5. **Monitor and optimize**: Use health checks and caching for best performance\n",
                "\n",
                "For more information, visit the [RefLex LLM documentation](https://github.com/your-repo/reflex-llm)."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
