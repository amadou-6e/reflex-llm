{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "aedae277",
            "metadata": {},
            "source": [
                "# RefLex LLM - Complete Azure OpenAI Integration Guide\n",
                "\n",
                "RefLex LLM is an intelligent Azure OpenAI API fallback system that automatically switches between Azure OpenAI, OpenAI, and local AI when endpoints become unavailable. It provides seamless failover capabilities while maintaining full Azure OpenAI API compatibility. The primary intent is to use the module for testing and CI runs, as local execution might be slower but also less expensive. In the future, with the possibility of spinning up a load balanced reflex kubernetes cluster, reflex could be shaped into a failsafe mechanism.\n",
                "\n",
                "## What is RefLex LLM?\n",
                "\n",
                "RefLex LLM acts as an intelligent middleware layer between your application and various AI providers. When your primary Azure OpenAI endpoint fails due to rate limits, outages, or network issues, RefLex automatically detects the failure and routes your requests to alternative providers without any code changes required.\n",
                "\n",
                "## Key Features\n",
                "\n",
                "- **Automatic Provider Selection**:  \n",
                "Intelligently chooses between Azure OpenAI, OpenAI, and local Ollama based on availability and your preferences\n",
                "- **Docker Integration**:  \n",
                "Automatically manages local AI containers with zero configuration\n",
                "- **Azure OpenAI Compatibility**:  \n",
                "Drop-in replacement for the Azure OpenAI Python client with identical API\n",
                "- **Model Mapping**:  \n",
                "Automatically maps Azure OpenAI deployment names to equivalent local models\n",
                "- **Configuration Management**:  \n",
                "Supports file-based configuration for different Azure environments\n",
                "- **Health Monitoring**:  \n",
                "Continuous health checking and automatic recovery\n",
                "- **Performance Optimization**:  \n",
                "Caches configurations and maintains persistent connections\n",
                "\n",
                "## Installation and Setup\n",
                "\n",
                "RefLex requires Python 3.8+ and Docker for local AI capabilities. The installation includes all necessary dependencies including the Azure OpenAI client, Docker SDK, and configuration management tools."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d67f6cfe",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install reflex-llms numpy"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "972e64cb",
            "metadata": {},
            "source": [
                "## Azure OpenAI Provider Resolution and Basic Usage\n",
                "\n",
                "RefLex automatically detects which AI providers are available and selects the best option based on your preference order. The system performs intelligent health checks by testing each provider in sequence and uses the first one that responds successfully.\n",
                "\n",
                "### How Provider Testing Works\n",
                "\n",
                "The provider resolution process involves several sophisticated steps:\n",
                "\n",
                "1. **Azure OpenAI Testing**: Makes test HTTP requests to your Azure OpenAI endpoint (e.g., https://your-resource.openai.azure.com/), checking for valid API responses and proper authentication\n",
                "\n",
                "2. **OpenAI Fallback**: If Azure is unavailable, tests the standard OpenAI endpoint as a fallback option\n",
                "\n",
                "3. **RefLex Local**: Automatically starts Docker containers if needed, manages Ollama installation, and verifies local model availability\n",
                "\n",
                "4. **Caching**: Successful configurations are cached to avoid repeated health checks and improve performance\n",
                "\n",
                "The system is designed to be resilient and will automatically retry failed providers and handle network timeouts gracefully.\n",
                "\n",
                "### Azure OpenAI Environment Setup\n",
                "\n",
                "For Azure OpenAI integration, ensure these environment variables are set:\n",
                "- `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI resource endpoint\n",
                "- `AZURE_OPENAI_API_KEY`: Your Azure OpenAI API key\n",
                "- `AZURE_OPENAI_DEPLOY_NAME`: Your deployment name (optional)\n",
                "- `AZURE_OPENAI_MODEL_NAME`: Your model name (optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e5afa4b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from reflex_llms import (\n",
                "    get_openai_client, \n",
                "    get_selected_provider,\n",
                "    get_module_status,\n",
                "    is_using_reflex\n",
                ")\n",
                "\n",
                "# Configure client with Azure preference\n",
                "client = get_openai_client(\n",
                "    preference_order=[\"azure\", \"reflex\"],  # Prefer Azure first\n",
                "    azure_api_version=\"2024-02-15-preview\",  # Use latest API version\n",
                "    azure_base_url=\"https://your-resource.openai.azure.com/\",  # Your Azure endpoint\n",
                "    timeout=10.0\n",
                ")\n",
                "\n",
                "# Display system status\n",
                "status = get_module_status()\n",
                "print(f\"Selected provider: {get_selected_provider()}\")\n",
                "print(f\"Using local RefLex: {is_using_reflex()}\")\n",
                "print(f\"Config cached: {status['has_cached_config']}\")\n",
                "print(f\"RefLex server running: {status['reflex_server_running']}\")\n",
                "\n",
                "if get_selected_provider() == \"azure\":\n",
                "    print(\"‚úÖ Successfully connected to Azure OpenAI\")\n",
                "elif get_selected_provider() == \"reflex\":\n",
                "    print(\"‚ö†Ô∏è Using RefLex fallback (Azure OpenAI not available)\")\n",
                "else:\n",
                "    print(f\"‚ÑπÔ∏è Using {get_selected_provider()} provider\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5494ebd",
            "metadata": {},
            "source": [
                "## Azure OpenAI Chat Completions with Automatic Failover\n",
                "\n",
                "RefLex provides identical Azure OpenAI API functionality regardless of the underlying provider. All standard Azure OpenAI parameters work seamlessly, including temperature, max_tokens, system messages, and advanced features. The client automatically handles provider differences behind the scenes, ensuring your application code remains unchanged.\n",
                "\n",
                "### Azure Deployment Names and Model Mapping\n",
                "\n",
                "When using Azure OpenAI, you typically work with deployment names rather than model names. RefLex handles this automatically, mapping your Azure deployment names to appropriate local models when failover occurs. For example, your \"gpt-35-turbo\" deployment might map to \"llama3.2:3b\" locally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de758e28",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import display utilities\n",
                "from utils import display_message, display_stream, display_embeddings\n",
                "\n",
                "# Standard Azure OpenAI chat completion\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-35-turbo\",  # Azure deployment name format\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Explain how RefLex LLM works with Azure OpenAI.\"}],\n",
                "    max_tokens=150,\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# Display formatted response\n",
                "display_message(response, as_markdown=True)\n",
                "\n",
                "print(f\"Model/Deployment used: {response.model}\")\n",
                "print(f\"Tokens: {response.usage.total_tokens if response.usage else 'Unknown'}\")\n",
                "print(f\"Provider: {get_selected_provider()}\")\n",
                "\n",
                "# Show Azure-specific information if using Azure\n",
                "if get_selected_provider() == \"azure\":\n",
                "    print(\"üî∑ Response served by Azure OpenAI\")\n",
                "    print(f\"API Version: 2024-02-15-preview\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ee7e404c",
            "metadata": {},
            "source": [
                "## Azure OpenAI Model Management and Deployment Mapping\n",
                "\n",
                "Azure OpenAI uses a deployment-based model where you create named deployments of specific models. RefLex intelligently handles the mapping between Azure deployment names and local model alternatives. This ensures seamless failover without requiring changes to your application code.\n",
                "\n",
                "### Common Azure Deployment Patterns\n",
                "\n",
                "Azure OpenAI deployments typically follow naming conventions like:\n",
                "- `gpt-35-turbo` for GPT-3.5 Turbo\n",
                "- `gpt-4` for GPT-4\n",
                "- `gpt-4-32k` for GPT-4 with extended context\n",
                "- `text-embedding-ada-002` for embeddings\n",
                "\n",
                "RefLex automatically maps these to compatible local models when Azure is unavailable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b9238152",
            "metadata": {},
            "outputs": [],
            "source": [
                "# List available models/deployments\n",
                "models = client.models.list()\n",
                "\n",
                "azure_deployments = []\n",
                "chat_models = []\n",
                "embedding_models = []\n",
                "gpt4_models = []\n",
                "\n",
                "for model in models.data:\n",
                "    model_id = model.id\n",
                "    if \"embedding\" in model_id or \"ada-002\" in model_id:\n",
                "        embedding_models.append(model_id)\n",
                "    elif \"gpt-4\" in model_id:\n",
                "        gpt4_models.append(model_id)\n",
                "    elif any(x in model_id for x in [\"gpt-35\", \"gpt-3.5\", \"turbo\"]):\n",
                "        chat_models.append(model_id)\n",
                "    \n",
                "    # Track Azure-style deployments\n",
                "    if get_selected_provider() == \"azure\":\n",
                "        azure_deployments.append(model_id)\n",
                "\n",
                "print(f\"Available models/deployments ({len(models.data)} total):\")\n",
                "\n",
                "if get_selected_provider() == \"azure\":\n",
                "    print(f\"üî∑ Azure OpenAI Deployments: {len(azure_deployments)}\")\n",
                "    print(f\"Sample deployments: {sorted(azure_deployments)[:3]}\")\n",
                "else:\n",
                "    print(f\"Chat models: {len(chat_models)}\")\n",
                "    print(f\"GPT-4 models: {len(gpt4_models)}\")\n",
                "    print(f\"Embedding models: {len(embedding_models)}\")\n",
                "    print(f\"Sample chat models: {sorted(chat_models)[:3]}\")\n",
                "    print(f\"Sample GPT-4 models: {sorted(gpt4_models)[:3]}\")\n",
                "    print(f\"Sample embedding models: {sorted(embedding_models)[:3]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b7fdcf7",
            "metadata": {},
            "source": [
                "## Azure OpenAI Embeddings with Cost Optimization\n",
                "\n",
                "Azure OpenAI embeddings are particularly valuable for enterprise applications where data sovereignty and regional compliance are important. RefLex provides automatic failover to local embedding models when Azure quotas are reached or during maintenance windows, ensuring continuous operation while maintaining cost efficiency.\n",
                "\n",
                "### Azure vs Local Embedding Trade-offs\n",
                "\n",
                "- **Azure Benefits**: Enterprise compliance, regional data residency, SLA guarantees, integration with Azure ecosystem\n",
                "- **Local Benefits**: No API costs for large volumes, no rate limits, offline capability, faster processing for batch operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "72e9ae69",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create embeddings using Azure OpenAI deployment\n",
                "embedding_response = client.embeddings.create(\n",
                "    model=\"text-embedding-ada-002\",  # Azure deployment name\n",
                "    input=\"RefLex LLM provides seamless fallback between Azure OpenAI and local AI models for enterprise applications.\"\n",
                ")\n",
                "\n",
                "# Display embedding information\n",
                "display_embeddings(embedding_response, show_stats=True)\n",
                "\n",
                "print(f\"Embedding provider: {get_selected_provider()}\")\n",
                "if get_selected_provider() == \"azure\":\n",
                "    print(\"üî∑ Embeddings generated by Azure OpenAI\")\n",
                "    print(\"‚úÖ Enterprise-grade compliance and data residency\")\n",
                "else:\n",
                "    print(\"‚ö° Embeddings generated locally (cost-optimized)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c7c590f4",
            "metadata": {},
            "source": [
                "## Azure OpenAI Streaming with Regional Optimization\n",
                "\n",
                "Azure OpenAI streaming provides enterprise-grade performance with regional optimization and compliance features. RefLex maintains identical streaming functionality regardless of provider, ensuring consistent user experience whether requests are served by Azure OpenAI or local fallback models.\n",
                "\n",
                "### Azure Streaming Benefits\n",
                "\n",
                "- **Regional Performance**: Azure's global infrastructure provides optimized latency\n",
                "- **Enterprise Features**: Built-in monitoring, logging, and compliance reporting\n",
                "- **SLA Guarantees**: Enterprise-grade availability and performance commitments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50759faf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create streaming request using Azure deployment\n",
                "stream = client.chat.completions.create(\n",
                "    model=\"gpt-35-turbo\",  # Azure deployment name\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Write a brief explanation of Azure OpenAI failover systems and enterprise benefits.\"}],\n",
                "    max_tokens=200,\n",
                "    stream=True,\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "print(f\"Streaming from: {get_selected_provider().upper()}\")\n",
                "if get_selected_provider() == \"azure\":\n",
                "    print(\"üî∑ Enterprise-grade Azure OpenAI streaming\")\n",
                "\n",
                "# Display streaming response\n",
                "full_response = display_stream(stream)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2db4cb39",
            "metadata": {},
            "source": [
                "## Azure OpenAI Advanced Models and Enterprise Features\n",
                "\n",
                "Azure OpenAI provides access to the latest models with enterprise-grade features including content filtering, abuse monitoring, and compliance reporting. RefLex seamlessly handles failover to local alternatives when Azure deployments are unavailable or rate-limited.\n",
                "\n",
                "### Azure OpenAI Enterprise Advantages\n",
                "\n",
                "- **Content Filtering**: Built-in content safety and compliance filtering\n",
                "- **Audit Logging**: Comprehensive logging for enterprise compliance\n",
                "- **Regional Deployment**: Data residency and regional compliance options\n",
                "- **SLA Support**: Enterprise-grade service level agreements\n",
                "- **Integration**: Seamless integration with Azure ecosystem and identity management"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ca01762",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test GPT-4 model with Azure deployment\n",
                "gpt4_stream = client.chat.completions.create(\n",
                "    model=\"gpt-4\",  # Azure GPT-4 deployment\n",
                "    messages=[{\"role\": \"user\", \"content\": \"A company migrating to Azure needs to handle 1000 concurrent users with 99.9% uptime. Design a failover strategy using Azure OpenAI and local backup systems.\"}],\n",
                "    max_tokens=300,\n",
                "    stream=True,\n",
                "    temperature=0.2  # Lower temperature for technical accuracy\n",
                ")\n",
                "\n",
                "print(f\"GPT-4 Response from: {get_selected_provider().upper()}\")\n",
                "if get_selected_provider() == \"azure\":\n",
                "    print(\"üî∑ Enterprise Azure OpenAI GPT-4\")\n",
                "    print(\"‚úÖ Content filtering and compliance enabled\")\n",
                "\n",
                "# Stream with markdown formatting\n",
                "gpt4_response = display_stream(gpt4_stream, as_markdown=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ce285d47",
            "metadata": {},
            "source": [
                "## Azure OpenAI Resource Management and Monitoring\n",
                "\n",
                "When using Azure OpenAI as the primary provider, RefLex provides visibility into both Azure resource utilization and local fallback capabilities. This dual-layer monitoring ensures you can optimize costs while maintaining service reliability.\n",
                "\n",
                "### Azure Resource Monitoring\n",
                "\n",
                "RefLex integrates with Azure's monitoring capabilities while providing additional insights into failover patterns, local resource usage, and cost optimization opportunities. This comprehensive view helps enterprises balance performance, cost, and compliance requirements."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2447c0f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from reflex_llms import get_reflex_server\n",
                "\n",
                "# Display provider-specific information\n",
                "current_provider = get_selected_provider()\n",
                "print(f\"Current Provider: {current_provider.upper()}\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "if current_provider == \"azure\":\n",
                "    print(\"üî∑ AZURE OPENAI CONFIGURATION\")\n",
                "    print(f\"‚Ä¢ API Version: 2024-02-15-preview\")\n",
                "    print(f\"‚Ä¢ Endpoint: [configured from environment]\")\n",
                "    print(f\"‚Ä¢ Authentication: Azure API Key\")\n",
                "    print(f\"‚Ä¢ Content Filtering: Enabled\")\n",
                "    print(f\"‚Ä¢ Enterprise Features: Available\")\n",
                "    print(f\"‚Ä¢ Regional Compliance: Configured\")\n",
                "    \n",
                "    # Check if RefLex backup is available\n",
                "    server = get_reflex_server()\n",
                "    if server:\n",
                "        print(f\"\\nüõ°Ô∏è BACKUP SYSTEM STATUS\")\n",
                "        print(f\"‚Ä¢ Local RefLex: Available\")\n",
                "        print(f\"‚Ä¢ Backup URL: {server.openai_compatible_url}\")\n",
                "        print(f\"‚Ä¢ Status: {server.is_healthy}\")\n",
                "    else:\n",
                "        print(f\"\\n‚ö†Ô∏è No local backup configured\")\n",
                "        print(f\"‚Ä¢ Consider enabling RefLex for high availability\")\n",
                "\n",
                "else:\n",
                "    # Access RefLex server instance for non-Azure providers\n",
                "    server = get_reflex_server()\n",
                "    \n",
                "    if server:\n",
                "        print(f\"‚ö° LOCAL REFLEX SERVER (Azure Fallback Active)\")\n",
                "        print(f\"‚Ä¢ API URL: {server.api_url}\")\n",
                "        print(f\"‚Ä¢ OpenAI Compatible URL: {server.openai_compatible_url}\")\n",
                "        print(f\"‚Ä¢ Container: {server.container_name}\")\n",
                "        print(f\"‚Ä¢ Status: {'üü¢ Healthy' if server.is_healthy else 'üî¥ Unhealthy'}\")\n",
                "        \n",
                "        # Detailed status\n",
                "        status = server.get_status()\n",
                "        print(f\"‚Ä¢ Total models: {status.get('total_models', 0)}\")\n",
                "        print(f\"‚Ä¢ OpenAI-compatible models: {len(status.get('openai_compatible_models', []))}\")\n",
                "        print(f\"‚Ä¢ Setup complete: {status.get('setup_complete', False)}\")\n",
                "        \n",
                "        print(f\"\\nüí° Azure OpenAI unavailable - using local backup\")\n",
                "    else:\n",
                "        print(f\"Using {current_provider} provider\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1173cc11",
            "metadata": {},
            "source": [
                "## Azure OpenAI Configuration Management\n",
                "\n",
                "RefLex supports sophisticated configuration management for Azure OpenAI environments, including multi-region deployments, environment-specific settings, and compliance configurations. This enables enterprises to maintain consistent AI capabilities across development, staging, and production environments.\n",
                "\n",
                "### Azure-Specific Configuration Features\n",
                "\n",
                "The configuration system provides Azure-specific options including:\n",
                "- Multiple Azure region configurations for geo-redundancy\n",
                "- Environment-specific API versions and deployment names\n",
                "- Content filtering and compliance policy settings\n",
                "- Cost optimization through intelligent provider selection\n",
                "- Integration with Azure Key Vault for secure credential management\n",
                "- Monitoring and alerting configuration for Azure resources"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62ff8f22",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Azure-optimized configuration example\n",
                "azure_config = {\n",
                "    \"_comment\": \"Azure OpenAI Optimized RefLex Configuration\",\n",
                "    \"preference_order\": [\"azure\", \"reflex\"],  # Azure first, local backup\n",
                "    \"azure_api_version\": \"2024-02-15-preview\",  # Latest Azure API\n",
                "    \"azure_base_url\": \"https://your-resource.openai.azure.com\",  # Your endpoint\n",
                "    \"timeout\": 15.0,  # Higher timeout for enterprise reliability\n",
                "    \"reflex_server\": {\n",
                "        \"host\": \"127.0.0.1\",\n",
                "        \"port\": 11434,\n",
                "        \"auto_setup\": True,\n",
                "        \"model_mappings\": {\n",
                "            \"minimal_setup\": False,  # Full model set for enterprise\n",
                "            \"model_mapping\": {\n",
                "                \"gpt-35-turbo\": \"llama3.2:3b\",  # Azure naming convention\n",
                "                \"gpt-4\": \"llama3.1:8b\",\n",
                "                \"gpt-4-32k\": \"llama3.1:70b\",\n",
                "                \"text-embedding-ada-002\": \"nomic-embed-text\"\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "from utils import display_json_inline\n",
                "print(\"Azure OpenAI Optimized Configuration:\")\n",
                "display_json_inline(azure_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8c9e49ef",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Environment-specific Azure configuration\n",
                "import os\n",
                "environment = os.getenv('ENVIRONMENT', 'development')\n",
                "\n",
                "# Azure region mapping for different environments\n",
                "azure_regions = {\n",
                "    'development': 'eastus',\n",
                "    'staging': 'westeurope', \n",
                "    'production': 'eastus2'\n",
                "}\n",
                "\n",
                "# Environment-specific provider preferences\n",
                "if environment == 'development':\n",
                "    preference = [\"reflex\", \"azure\"]  # Local first for dev\n",
                "    api_version = \"2024-02-15-preview\"  # Latest for testing\n",
                "elif environment == 'production':\n",
                "    preference = [\"azure\", \"reflex\"]  # Azure first for prod\n",
                "    api_version = \"2024-02-01\"  # Stable version for prod\n",
                "else:\n",
                "    preference = [\"azure\", \"reflex\"]\n",
                "    api_version = \"2024-02-15-preview\"\n",
                "\n",
                "print(f\"Environment: {environment}\")\n",
                "print(f\"Azure Region: {azure_regions.get(environment, 'eastus')}\")\n",
                "print(f\"Provider Preference: {preference}\")\n",
                "print(f\"Azure API Version: {api_version}\")\n",
                "print(f\"Content Filtering: {'Strict' if environment == 'production' else 'Standard'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "496ed317",
            "metadata": {},
            "source": [
                "## Azure OpenAI Enterprise Error Handling and Compliance\n",
                "\n",
                "RefLex provides enterprise-grade error handling specifically designed for Azure OpenAI deployments, including compliance logging, audit trails, and automated incident response. The system gracefully handles Azure-specific scenarios like quota limits, content filtering blocks, and regional failover.\n",
                "\n",
                "### Azure-Specific Error Scenarios\n",
                "\n",
                "- **Azure Quota Exhaustion**: Automatic failover to local models when Azure quotas are reached\n",
                "- **Content Filter Blocks**: Intelligent retry logic and alternative model selection\n",
                "- **Regional Outages**: Multi-region failover with automatic recovery\n",
                "- **Authentication Issues**: Secure credential refresh and backup authentication\n",
                "- **Compliance Violations**: Audit logging and automated compliance reporting\n",
                "\n",
                "### Enterprise Monitoring and Diagnostics\n",
                "\n",
                "The diagnostic system provides comprehensive visibility into Azure OpenAI resource utilization, compliance status, and failover patterns. This information is essential for enterprise operations, cost optimization, and regulatory compliance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71a45781",
            "metadata": {},
            "outputs": [],
            "source": [
                "from reflex_llms import clear_cache, stop_reflex_server\n",
                "import os\n",
                "\n",
                "# Azure OpenAI specific diagnostics\n",
                "final_status = get_module_status()\n",
                "current_provider = get_selected_provider()\n",
                "\n",
                "print(\"üî∑ AZURE OPENAI SYSTEM DIAGNOSTICS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Active Provider: {current_provider.upper()}\")\n",
                "print(f\"Configuration Cached: {final_status['has_cached_config']}\")\n",
                "print(f\"RefLex Backup Available: {final_status['reflex_server_running']}\")\n",
                "\n",
                "# Azure environment validation\n",
                "print(f\"\\nüîê AZURE CREDENTIALS STATUS\")\n",
                "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
                "azure_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
                "azure_deployment = os.getenv('AZURE_OPENAI_DEPLOY_NAME')\n",
                "azure_model = os.getenv('AZURE_OPENAI_MODEL_NAME')\n",
                "\n",
                "print(f\"‚Ä¢ Endpoint: {'‚úÖ Configured' if azure_endpoint else '‚ùå Missing'}\")\n",
                "print(f\"‚Ä¢ API Key: {'‚úÖ Configured' if azure_key else '‚ùå Missing'}\")\n",
                "print(f\"‚Ä¢ Deployment Name: {'‚úÖ Configured' if azure_deployment else '‚ö†Ô∏è Optional'}\")\n",
                "print(f\"‚Ä¢ Model Name: {'‚úÖ Configured' if azure_model else '‚ö†Ô∏è Optional'}\")\n",
                "\n",
                "# Provide Azure-specific guidance\n",
                "print(f\"\\nüí° OPTIMIZATION RECOMMENDATIONS\")\n",
                "if current_provider == \"azure\":\n",
                "    print(f\"‚Ä¢ Azure OpenAI is active and healthy\")\n",
                "    print(f\"‚Ä¢ Consider enabling RefLex backup for high availability\")\n",
                "    print(f\"‚Ä¢ Monitor Azure quotas and usage patterns\")\n",
                "    print(f\"‚Ä¢ Implement content filtering policies as needed\")\n",
                "elif current_provider == \"reflex\":\n",
                "    print(f\"‚Ä¢ Using local backup - Azure OpenAI may be unavailable\")\n",
                "    print(f\"‚Ä¢ Check Azure credentials and endpoint connectivity\")\n",
                "    print(f\"‚Ä¢ Verify Azure resource quotas and billing status\")\n",
                "    print(f\"‚Ä¢ Consider multi-region Azure deployment for redundancy\")\n",
                "else:\n",
                "    print(f\"‚Ä¢ Using {current_provider} - configure Azure for enterprise features\")\n",
                "\n",
                "# Management commands\n",
                "print(f\"\\nüõ†Ô∏è MANAGEMENT COMMANDS\")\n",
                "print(f\"‚Ä¢ clear_cache() - Force provider re-resolution\")\n",
                "print(f\"‚Ä¢ stop_reflex_server() - Clean up local resources\")\n",
                "print(f\"‚Ä¢ Use preference_order=['azure', 'reflex'] for Azure-first setup\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
