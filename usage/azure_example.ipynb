{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RefLex LLM - Azure OpenAI Integration Example\n",
                "\n",
                "This notebook demonstrates how to use RefLex LLM specifically with Azure OpenAI endpoints, including configuration, fallback capabilities, and best practices.\n",
                "\n",
                "## Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install reflex-llms"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Azure OpenAI Setup\n",
                "\n",
                "### Environment Variables for Azure"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading environment from '.env'...\n",
                        "Missing required variables: AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, AZURE_OPENAI_API_VERSION\n",
                        "\n",
                        "Contents of 'c:\\Users\\acisse\\CodeWorkspace\\reflex-llm\\.env':\n",
                        "----------------------------------------\n",
                        " 1: OPENAI_API_KEY=sk****************************************************************************************************************************************************************wA\n",
                        "----------------------------------------\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "False"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from utils import load_and_verify_env\n",
                "\n",
                "load_and_verify_env(required_vars=[\n",
                "    'AZURE_OPENAI_ENDPOINT',\n",
                "    'AZURE_OPENAI_API_KEY',\n",
                "    'AZURE_OPENAI_API_VERSION',\n",
                "])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Basic Azure OpenAI Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "!netstat -aon | findstr :11435"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Checking OpenAI API providers...\n",
                        "  Trying azure...\n",
                        "  Trying reflex...\n",
                        "  Setting up RefLex server...\n",
                        "Setting up RefLex OpenAI-compatible backend...\n",
                        "Starting Ollama container...\n",
                        "Ollama is already running and ready\n",
                        "Waiting for Ollama to be ready...\n",
                        "Ollama ready! Found 16 existing models\n",
                        "Setting up OpenAI model mappings...\n",
                        "Setting up OpenAI-compatible models...\n",
                        "\n",
                        "Processing: gpt-3.5-turbo -> llama3.2:3b\n",
                        "Model gpt-3.5-turbo already exists, skipping...\n",
                        "\n",
                        "Processing: gpt-3.5-turbo-16k -> llama3.2:3b\n",
                        "Model gpt-3.5-turbo-16k already exists, skipping...\n",
                        "\n",
                        "Processing: gpt-4 -> llama3.1:8b\n",
                        "Model gpt-4 already exists, skipping...\n",
                        "\n",
                        "Processing: gpt-4-turbo -> gemma3:4b\n",
                        "Model gpt-4-turbo already exists, skipping...\n",
                        "\n",
                        "Processing: gpt-4o -> gemma3:4b\n",
                        "Model gpt-4o already exists, skipping...\n",
                        "\n",
                        "Processing: gpt-4o-mini -> gemma3:1b\n",
                        "Model gpt-4o-mini already exists, skipping...\n",
                        "\n",
                        "Processing: o1-preview -> phi3:reasoning\n",
                        "Pulling model: phi3:reasoning\n",
                        "Failed to pull model phi3:reasoning: Error pulling model: pull model manifest: file does not exist\n",
                        "Failed to pull phi3:reasoning, skipping o1-preview\n",
                        "\n",
                        "Processing: o1-mini -> phi3:reasoning-mini\n",
                        "Pulling model: phi3:reasoning-mini\n",
                        "Failed to pull model phi3:reasoning-mini: Error pulling model: pull model manifest: file does not exist\n",
                        "Failed to pull phi3:reasoning-mini, skipping o1-mini\n",
                        "\n",
                        "Processing: o3-mini -> phi3:reasoning-mini\n",
                        "Pulling model: phi3:reasoning-mini\n",
                        "Failed to pull model phi3:reasoning-mini: Error pulling model: pull model manifest: file does not exist\n",
                        "Failed to pull phi3:reasoning-mini, skipping o3-mini\n",
                        "\n",
                        "Processing: o3 -> phi3:reasoning\n",
                        "Pulling model: phi3:reasoning\n",
                        "Failed to pull model phi3:reasoning: Error pulling model: pull model manifest: file does not exist\n",
                        "Failed to pull phi3:reasoning, skipping o3\n",
                        "\n",
                        "Processing: o4-mini -> phi3:reasoning-mini\n",
                        "Pulling model: phi3:reasoning-mini\n",
                        "Failed to pull model phi3:reasoning-mini: Error pulling model: pull model manifest: file does not exist\n",
                        "Failed to pull phi3:reasoning-mini, skipping o4-mini\n",
                        "\n",
                        "Processing: text-embedding-ada-002 -> nomic-embed-text\n",
                        "Model text-embedding-ada-002 already exists, skipping...\n",
                        "\n",
                        "Processing: text-embedding-3-small -> nomic-embed-text\n",
                        "Model text-embedding-3-small already exists, skipping...\n",
                        "\n",
                        "Processing: text-embedding-3-large -> mxbai-embed-large\n",
                        "Model text-embedding-3-large already exists, skipping...\n",
                        "\n",
                        "Model setup complete: 9/14 models configured\n",
                        "Warning: Some models failed to setup, but backend is functional\n",
                        "Performing final health check...\n",
                        "RefLex OpenAI backend setup complete!\n",
                        "OpenAI-compatible endpoint: http://127.0.0.1:11434/v1\n",
                        "Status endpoint: http://127.0.0.1:11434/api/tags\n",
                        "Response: Hello! It sounds like you're leveraging Azure OpenAI's capabilities through the Reflex framework. How can I assist you with your query or question? Are you looking to use the OpenAI API, get insights on how to optimize your model, or perhaps explore a specific feature of the platform? Feel free to share more about what you need help with!\n",
                        "Using provider: reflex\n"
                    ]
                }
            ],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Get client with Azure preference\n",
                "client = reflex_llms.get_openai_client(\n",
                "    preference_order=[\"azure\", \"reflex\"],  # Try Azure first, fallback to local\n",
                "    port=11435\n",
                ")\n",
                "\n",
                "# Use exactly like the OpenAI client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",  # Azure model deployment name\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"Hello! I'm using Azure OpenAI through RefLex.\"}\n",
                "    ],\n",
                "    max_tokens=100\n",
                ")\n",
                "\n",
                "print(f\"Response: {response.choices[0].message.content}\")\n",
                "print(f\"Using provider: {reflex_llms.get_selected_provider()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Azure-Specific Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Custom Azure configuration\n",
                "client = reflex_llms.get_openai_client(\n",
                "    preference_order=[\"azure\"],\n",
                "    azure_api_version=\"2024-06-01\",  # Specific API version\n",
                "    azure_base_url=\"https://custom-azure.openai.azure.com\",  # Custom endpoint\n",
                "    timeout=15.0\n",
                ")\n",
                "\n",
                "print(f\"Azure client configured with provider: {reflex_llms.get_selected_provider()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Azure Configuration File Example\n",
                "\n",
                "Create a `reflex.json` file optimized for Azure:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# Azure-focused configuration\n",
                "azure_config = {\n",
                "    \"preference_order\": [\"azure\", \"reflex\"],\n",
                "    \"timeout\": 20.0,\n",
                "    \"azure\": {\n",
                "        \"api_version\": \"2024-06-01\",\n",
                "        \"base_url\": \"https://your-resource.openai.azure.com\"\n",
                "    },\n",
                "    \"reflex_server_config\": {\n",
                "        \"port\": 8080,\n",
                "        \"container_name\": \"azure-fallback-server\",\n",
                "        \"model_mappings\": {\n",
                "            \"minimal_setup\": True,\n",
                "            \"minimal_model_mapping\": {\n",
                "                \"gpt-35-turbo\": \"llama3.2:3b\",\n",
                "                \"gpt-4\": \"llama3.1:8b\",\n",
                "                \"text-embedding-ada-002\": \"nomic-embed-text\"\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save configuration\n",
                "with open('reflex_azure.json', 'w') as f:\n",
                "    json.dump(azure_config, f, indent=2)\n",
                "\n",
                "print(\"Azure configuration saved to reflex_azure.json\")\n",
                "print(json.dumps(azure_config, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chat Application with Azure Fallback"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "def azure_chat_with_fallback(user_input=\"Hello from Azure!\"):\n",
                "    \"\"\"Chat function that prefers Azure but falls back to local AI.\"\"\"\n",
                "    \n",
                "    # Load from Azure config file\n",
                "    client = reflex_llms.get_openai_client(from_file=\"reflex_azure.json\")\n",
                "    \n",
                "    provider = reflex_llms.get_selected_provider()\n",
                "    print(f\"Chat ready! Using {provider}\")\n",
                "    \n",
                "    conversation = [{\"role\": \"user\", \"content\": user_input}]\n",
                "    \n",
                "    try:\n",
                "        # Use Azure model deployment names\n",
                "        model_name = \"gpt-35-turbo\" if provider == \"azure\" else \"gpt-3.5-turbo\"\n",
                "        \n",
                "        response = client.chat.completions.create(\n",
                "            model=model_name,\n",
                "            messages=conversation,\n",
                "            max_tokens=150\n",
                "        )\n",
                "        \n",
                "        ai_response = response.choices[0].message.content\n",
                "        print(f\"You: {user_input}\")\n",
                "        print(f\"AI ({provider}): {ai_response}\")\n",
                "        \n",
                "        return ai_response\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error with {provider}: {e}\")\n",
                "        print(\"Attempting fallback...\")\n",
                "        \n",
                "        # Clear cache and try fallback\n",
                "        reflex_llms.clear_cache()\n",
                "        client = reflex_llms.get_openai_client(from_file=\"reflex_azure.json\")\n",
                "        \n",
                "        new_provider = reflex_llms.get_selected_provider()\n",
                "        print(f\"Switched to: {new_provider}\")\n",
                "        \n",
                "        return \"Successfully switched to fallback provider!\"\n",
                "\n",
                "# Test the chat with fallback\n",
                "response = azure_chat_with_fallback(\"Explain the benefits of using Azure OpenAI\")\n",
                "print(f\"\\nResponse length: {len(response)} characters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Azure Embeddings with Fallback"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "def get_azure_embeddings(texts):\n",
                "    \"\"\"Generate embeddings using Azure OpenAI with local fallback.\"\"\"\n",
                "    client = reflex_llms.get_openai_client(from_file=\"reflex_azure.json\")\n",
                "    \n",
                "    if isinstance(texts, str):\n",
                "        texts = [texts]\n",
                "    \n",
                "    provider = reflex_llms.get_selected_provider()\n",
                "    \n",
                "    # Azure uses deployment names, local uses model names\n",
                "    embedding_model = (\n",
                "        \"text-embedding-ada-002\" if provider == \"azure\" \n",
                "        else \"text-embedding-ada-002\"  # Same for both in this case\n",
                "    )\n",
                "    \n",
                "    embeddings = []\n",
                "    \n",
                "    for text in texts:\n",
                "        try:\n",
                "            response = client.embeddings.create(\n",
                "                model=embedding_model,\n",
                "                input=text\n",
                "            )\n",
                "            embeddings.append(response.data[0].embedding)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Embedding failed with {provider}: {e}\")\n",
                "            raise\n",
                "    \n",
                "    return {\n",
                "        \"embeddings\": embeddings,\n",
                "        \"provider\": provider,\n",
                "        \"count\": len(embeddings)\n",
                "    }\n",
                "\n",
                "# Example usage\n",
                "sample_texts = [\n",
                "    \"Azure OpenAI provides enterprise-grade AI services\",\n",
                "    \"RefLex enables seamless fallback to local AI servers\",\n",
                "    \"Hybrid AI deployment strategies improve reliability\"\n",
                "]\n",
                "\n",
                "try:\n",
                "    result = get_azure_embeddings(sample_texts)\n",
                "    print(f\"Generated {result['count']} embeddings using {result['provider']}\")\n",
                "    print(f\"First embedding dimension: {len(result['embeddings'][0])}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Embedding generation failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Azure Monitoring and Troubleshooting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "import time\n",
                "\n",
                "def monitor_azure_health():\n",
                "    \"\"\"Comprehensive Azure OpenAI health monitoring.\"\"\"\n",
                "    \n",
                "    print(\"=== Azure OpenAI Health Report ===\")\n",
                "    \n",
                "    # Get current status\n",
                "    status = reflex_llms.get_module_status()\n",
                "    provider = status['selected_provider']\n",
                "    \n",
                "    print(f\"Current Provider: {provider}\")\n",
                "    \n",
                "    if provider == \"azure\":\n",
                "        print(\"✅ Azure OpenAI is active\")\n",
                "        \n",
                "        # Test Azure API performance\n",
                "        try:\n",
                "            start_time = time.time()\n",
                "            client = reflex_llms.get_openai_client()\n",
                "            \n",
                "            response = client.chat.completions.create(\n",
                "                model=\"gpt-35-turbo\",\n",
                "                messages=[{\"role\": \"user\", \"content\": \"ping\"}],\n",
                "                max_tokens=1\n",
                "            )\n",
                "            \n",
                "            response_time = time.time() - start_time\n",
                "            print(f\"Azure Response Time: {response_time:.2f}s\")\n",
                "            print(\"Azure API Status: Healthy ✅\")\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Azure API Error: {e} ❌\")\n",
                "            \n",
                "    elif provider == \"reflex\":\n",
                "        print(\"⚠️  Using local fallback - Azure unavailable\")\n",
                "        \n",
                "        server = reflex_llms.get_reflex_server()\n",
                "        if server:\n",
                "            server_status = server.get_status()\n",
                "            print(f\"Fallback Server: Running ✅\")\n",
                "            print(f\"Available Models: {len(server_status.get('openai_compatible_models', []))}\")\n",
                "        else:\n",
                "            print(\"Fallback Server: Not available ❌\")\n",
                "    \n",
                "    else:\n",
                "        print(f\"Unexpected provider: {provider} ⚠️\")\n",
                "    \n",
                "    # Configuration check\n",
                "    print(f\"\\nConfiguration:\")\n",
                "    print(f\"  Config Cached: {status['has_cached_config']}\")\n",
                "    print(f\"  Fallback Ready: {status.get('reflex_server_running', False)}\")\n",
                "    \n",
                "    return status\n",
                "\n",
                "# Run Azure health monitoring\n",
                "health_status = monitor_azure_health()\n",
                "\n",
                "# Force fallback test\n",
                "print(\"\\n=== Testing Fallback Mechanism ===\")\n",
                "reflex_llms.clear_cache()\n",
                "\n",
                "# Try with reflex priority to test fallback\n",
                "try:\n",
                "    fallback_client = reflex_llms.get_openai_client(\n",
                "        preference_order=[\"reflex\", \"azure\"]\n",
                "    )\n",
                "    fallback_provider = reflex_llms.get_selected_provider()\n",
                "    print(f\"Fallback test provider: {fallback_provider}\")\n",
                "except Exception as e:\n",
                "    print(f\"Fallback test failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "\n",
                "1. **Azure OpenAI Integration** - Primary Azure configuration with environment variables\n",
                "2. **Intelligent Fallback** - Automatic switching to local AI when Azure is unavailable\n",
                "3. **Configuration Management** - Azure-specific settings and deployment mappings\n",
                "4. **Production Deployment** - Enterprise-ready Azure setup with monitoring\n",
                "5. **Health Monitoring** - Comprehensive Azure service monitoring and troubleshooting\n",
                "\n",
                "Key benefits for Azure users:\n",
                "- **Enterprise Compliance** - Use Azure's enterprise-grade infrastructure\n",
                "- **Cost Control** - Fallback to local AI during outages or budget limits\n",
                "- **Reliability** - Automatic failover ensures continuous operation\n",
                "- **Flexibility** - Easy switching between Azure regions or deployments\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
