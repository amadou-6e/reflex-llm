{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RefLex LLM - OpenAI Integration Example\n",
                "\n",
                "This notebook demonstrates how to use RefLex LLM specifically with OpenAI endpoints, including configuration, fallback capabilities, and best practices.\n",
                "\n",
                "## Installation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Best Practices for OpenAI Integration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "class OpenAIBestPractices:\n",
                "    \"\"\"Demonstrates best practices for OpenAI integration with RefLex.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.client = reflex_llms.get_openai_client(from_file=\"reflex_openai.json\")\n",
                "        self.request_count = 0\n",
                "        self.total_tokens = 0\n",
                "    \n",
                "    def rate_limited_request(self, messages, model=\"gpt-3.5-turbo\", **kwargs):\n",
                "        \"\"\"Make requests with basic rate limiting awareness.\"\"\"\n",
                "        import time\n",
                "        \n",
                "        # Simple rate limiting (adjust based on your OpenAI tier)\n",
                "        if self.request_count > 0 and self.request_count % 10 == 0:\n",
                "            print(\"Rate limiting: brief pause...\")\n",
                "            time.sleep(1)\n",
                "        \n",
                "        try:\n",
                "            response = self.client.chat.completions.create(\n",
                "                model=model,\n",
                "                messages=messages,\n",
                "                **kwargs\n",
                "            )\n",
                "            \n",
                "            self.request_count += 1\n",
                "            if hasattr(response, 'usage'):\n",
                "                self.total_tokens += response.usage.total_tokens\n",
                "            \n",
                "            return response\n",
                "            \n",
                "        except Exception as e:\n",
                "            if \"rate_limit\" in str(e).lower():\n",
                "                print(\"Rate limit hit, switching to fallback...\")\n",
                "                reflex_llms.clear_cache()\n",
                "                self.client = reflex_llms.get_openai_client(\n",
                "                    preference_order=[\"reflex\", \"openai\"]\n",
                "                )\n",
                "                return self.client.chat.completions.create(\n",
                "                    model=model,\n",
                "                    messages=messages,\n",
                "                    **kwargs\n",
                "                )\n",
                "            else:\n",
                "                raise e\n",
                "    \n",
                "    def optimized_prompt(self, user_query):\n",
                "        \"\"\"Example of prompt optimization for better results.\"\"\"\n",
                "        system_prompt = \"\"\"\n",
                "You are a helpful AI assistant. Be concise but comprehensive.\n",
                "If you're unsure about something, say so clearly.\n",
                "Structure your responses with clear sections when appropriate.\n",
                "\"\"\"\n",
                "        \n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
                "            {\"role\": \"user\", \"content\": user_query}\n",
                "        ]\n",
                "        \n",
                "        return self.rate_limited_request(\n",
                "            messages=messages,\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            max_tokens=300,\n",
                "            temperature=0.7\n",
                "        )\n",
                "    \n",
                "    def batch_process(self, queries, batch_size=5):\n",
                "        \"\"\"Process multiple queries efficiently.\"\"\"\n",
                "        results = []\n",
                "        \n",
                "        for i in range(0, len(queries), batch_size):\n",
                "            batch = queries[i:i + batch_size]\n",
                "            batch_results = []\n",
                "            \n",
                "            for query in batch:\n",
                "                try:\n",
                "                    response = self.optimized_prompt(query)\n",
                "                    batch_results.append({\n",
                "                        \"query\": query,\n",
                "                        \"response\": response.choices[0].message.content,\n",
                "                        \"success\": True\n",
                "                    })\n",
                "                except Exception as e:\n",
                "                    batch_results.append({\n",
                "                        \"query\": query,\n",
                "                        \"error\": str(e),\n",
                "                        \"success\": False\n",
                "                    })\n",
                "            \n",
                "            results.extend(batch_results)\n",
                "            \n",
                "            # Brief pause between batches\n",
                "            if i + batch_size < len(queries):\n",
                "                import time\n",
                "                time.sleep(0.5)\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def get_stats(self):\n",
                "        \"\"\"Get usage statistics.\"\"\"\n",
                "        return {\n",
                "            \"requests_made\": self.request_count,\n",
                "            \"total_tokens\": self.total_tokens,\n",
                "            \"current_provider\": reflex_llms.get_selected_provider()\n",
                "        }\n",
                "\n",
                "# Example usage of best practices\n",
                "bp = OpenAIBestPractices()\n",
                "\n",
                "# Test optimized prompting\n",
                "response = bp.optimized_prompt(\"What are the key advantages of using OpenAI models?\")\n",
                "print(f\"Optimized response: {response.choices[0].message.content[:100]}...\")\n",
                "\n",
                "# Test batch processing\n",
                "test_queries = [\n",
                "    \"What is machine learning?\",\n",
                "    \"Explain neural networks briefly\",\n",
                "    \"What is the difference between AI and ML?\"\n",
                "]\n",
                "\n",
                "batch_results = bp.batch_process(test_queries)\n",
                "print(f\"\\nBatch processing results: {len(batch_results)} queries processed\")\n",
                "for result in batch_results:\n",
                "    if result[\"success\"]:\n",
                "        print(f\"✅ {result['query'][:30]}... -> {result['response'][:50]}...\")\n",
                "    else:\n",
                "        print(f\"❌ {result['query'][:30]}... -> Error: {result['error'][:30]}...\")\n",
                "\n",
                "# Show statistics\n",
                "stats = bp.get_stats()\n",
                "print(f\"\\nUsage Statistics: {stats}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "\n",
                "1. **OpenAI Integration** - Primary OpenAI configuration with API keys\n",
                "2. **Intelligent Fallback** - Automatic switching to local AI when OpenAI is unavailable\n",
                "3. **Model Flexibility** - Testing multiple OpenAI models (GPT-3.5, GPT-4, GPT-4o)\n",
                "4. **Production Setup** - Enterprise-ready OpenAI deployment with monitoring\n",
                "5. **Best Practices** - Rate limiting, prompt optimization, and batch processing\n",
                "6. **Streaming Support** - Real-time response streaming capabilities\n",
                "7. **Cost Management** - Usage tracking and cost estimation features\n",
                "\n",
                "Key benefits for OpenAI users:\n",
                "- **Reliability** - Never lose AI capability even during OpenAI outages\n",
                "- **Cost Control** - Fallback to local AI during rate limits or budget constraints\n",
                "- **Performance** - Optimized request handling and batch processing\n",
                "- **Flexibility** - Easy switching between different OpenAI models and configurations\n",
                "- **Monitoring** - Comprehensive health checks and usage tracking\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install reflex-llms"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## OpenAI Setup\n",
                "\n",
                "### Environment Variables for OpenAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "# OpenAI API credentials\n",
                "os.environ['OPENAI_API_KEY'] = 'your-openai-api-key'\n",
                "# Optional: Custom OpenAI base URL for enterprise\n",
                "# os.environ['OPENAI_BASE_URL'] = 'https://enterprise-api.openai.com/v1'\n",
                "\n",
                "print(\"OpenAI environment setup complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Basic OpenAI Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Get client with OpenAI preference\n",
                "client = reflex_llms.get_openai_client(\n",
                "    preference_order=[\"openai\", \"reflex\"]  # Try OpenAI first, fallback to local\n",
                ")\n",
                "\n",
                "# Use exactly like the OpenAI client\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\"role\": \"user\", \"content\": \"Hello! I'm using OpenAI through RefLex.\"}\n",
                "    ],\n",
                "    max_tokens=100\n",
                ")\n",
                "\n",
                "print(f\"Response: {response.choices[0].message.content}\")\n",
                "print(f\"Using provider: {reflex_llms.get_selected_provider()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## OpenAI-Specific Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "# Custom OpenAI configuration\n",
                "client = reflex_llms.get_openai_client(\n",
                "    preference_order=[\"openai\"],\n",
                "    openai_base_url=\"https://api.openai.com/v1\",  # Standard or custom endpoint\n",
                "    timeout=10.0\n",
                ")\n",
                "\n",
                "print(f\"OpenAI client configured with provider: {reflex_llms.get_selected_provider()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## OpenAI Configuration File Example\n",
                "\n",
                "Create a `reflex.json` file optimized for OpenAI:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "# OpenAI-focused configuration\n",
                "openai_config = {\n",
                "    \"preference_order\": [\"openai\", \"reflex\"],\n",
                "    \"timeout\": 15.0,\n",
                "    \"openai\": {\n",
                "        \"base_url\": \"https://api.openai.com/v1\"\n",
                "    },\n",
                "    \"reflex_server_config\": {\n",
                "        \"port\": 8080,\n",
                "        \"container_name\": \"openai-fallback-server\",\n",
                "        \"model_mappings\": {\n",
                "            \"minimal_setup\": True,\n",
                "            \"minimal_model_mapping\": {\n",
                "                \"gpt-3.5-turbo\": \"llama3.2:3b\",\n",
                "                \"gpt-4\": \"llama3.1:8b\",\n",
                "                \"gpt-4o\": \"llama3.1:70b\",\n",
                "                \"gpt-4o-mini\": \"gemma3:2b\",\n",
                "                \"text-embedding-ada-002\": \"nomic-embed-text\"\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save configuration\n",
                "with open('reflex_openai.json', 'w') as f:\n",
                "    json.dump(openai_config, f, indent=2)\n",
                "\n",
                "print(\"OpenAI configuration saved to reflex_openai.json\")\n",
                "print(json.dumps(openai_config, indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Chat Application with OpenAI Fallback"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "def openai_chat_with_fallback(user_input=\"Hello from OpenAI!\"):\n",
                "    \"\"\"Chat function that prefers OpenAI but falls back to local AI.\"\"\"\n",
                "    \n",
                "    # Load from OpenAI config file\n",
                "    client = reflex_llms.get_openai_client(from_file=\"reflex_openai.json\")\n",
                "    \n",
                "    provider = reflex_llms.get_selected_provider()\n",
                "    print(f\"Chat ready! Using {provider}\")\n",
                "    \n",
                "    conversation = [{\"role\": \"user\", \"content\": user_input}]\n",
                "    \n",
                "    try:\n",
                "        # OpenAI uses standard model names\n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=conversation,\n",
                "            max_tokens=150\n",
                "        )\n",
                "        \n",
                "        ai_response = response.choices[0].message.content\n",
                "        print(f\"You: {user_input}\")\n",
                "        print(f\"AI ({provider}): {ai_response}\")\n",
                "        \n",
                "        return ai_response\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Error with {provider}: {e}\")\n",
                "        print(\"Attempting fallback...\")\n",
                "        \n",
                "        # Clear cache and try fallback\n",
                "        reflex_llms.clear_cache()\n",
                "        client = reflex_llms.get_openai_client(from_file=\"reflex_openai.json\")\n",
                "        \n",
                "        new_provider = reflex_llms.get_selected_provider()\n",
                "        print(f\"Switched to: {new_provider}\")\n",
                "        \n",
                "        return \"Successfully switched to fallback provider!\"\n",
                "\n",
                "# Test the chat with fallback\n",
                "response = openai_chat_with_fallback(\"Explain the benefits of using OpenAI models\")\n",
                "print(f\"\\nResponse length: {len(response)} characters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Advanced OpenAI Model Usage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "def test_multiple_openai_models():\n",
                "    \"\"\"Test different OpenAI models with fallback.\"\"\"\n",
                "    client = reflex_llms.get_openai_client(from_file=\"reflex_openai.json\")\n",
                "    \n",
                "    models_to_test = [\n",
                "        \"gpt-3.5-turbo\",\n",
                "        \"gpt-4o-mini\",\n",
                "        \"gpt-4\",  # Will fallback if not available\n",
                "        \"gpt-4o\"   # Will fallback if not available\n",
                "    ]\n",
                "    \n",
                "    results = {}\n",
                "    \n",
                "    for model in models_to_test:\n",
                "        try:\n",
                "            response = client.chat.completions.create(\n",
                "                model=model,\n",
                "                messages=[\n",
                "                    {\"role\": \"user\", \"content\": f\"Say hello using {model}\"}\n",
                "                ],\n",
                "                max_tokens=30\n",
                "            )\n",
                "            \n",
                "            results[model] = {\n",
                "                \"success\": True,\n",
                "                \"response\": response.choices[0].message.content,\n",
                "                \"provider\": reflex_llms.get_selected_provider()\n",
                "            }\n",
                "            \n",
                "        except Exception as e:\n",
                "            results[model] = {\n",
                "                \"success\": False,\n",
                "                \"error\": str(e),\n",
                "                \"provider\": reflex_llms.get_selected_provider()\n",
                "            }\n",
                "    \n",
                "    return results\n",
                "\n",
                "# Test multiple models\n",
                "model_results = test_multiple_openai_models()\n",
                "\n",
                "print(\"OpenAI Model Test Results:\")\n",
                "print(\"===========================\")\n",
                "for model, result in model_results.items():\n",
                "    if result[\"success\"]:\n",
                "        print(f\"✅ {model} ({result['provider']}): {result['response'][:50]}...\")\n",
                "    else:\n",
                "        print(f\"❌ {model}: {result['error'][:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## OpenAI Embeddings with Fallback"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "def get_openai_embeddings(texts):\n",
                "    \"\"\"Generate embeddings using OpenAI with local fallback.\"\"\"\n",
                "    client = reflex_llms.get_openai_client(from_file=\"reflex_openai.json\")\n",
                "    \n",
                "    if isinstance(texts, str):\n",
                "        texts = [texts]\n",
                "    \n",
                "    provider = reflex_llms.get_selected_provider()\n",
                "    embedding_model = \"text-embedding-ada-002\"\n",
                "    \n",
                "    embeddings = []\n",
                "    \n",
                "    for text in texts:\n",
                "        try:\n",
                "            response = client.embeddings.create(\n",
                "                model=embedding_model,\n",
                "                input=text\n",
                "            )\n",
                "            embeddings.append(response.data[0].embedding)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Embedding failed with {provider}: {e}\")\n",
                "            # Try fallback\n",
                "            reflex_llms.clear_cache()\n",
                "            client = reflex_llms.get_openai_client(from_file=\"reflex_openai.json\")\n",
                "            new_provider = reflex_llms.get_selected_provider()\n",
                "            print(f\"Retrying with {new_provider}...\")\n",
                "            \n",
                "            response = client.embeddings.create(\n",
                "                model=embedding_model,\n",
                "                input=text\n",
                "            )\n",
                "            embeddings.append(response.data[0].embedding)\n",
                "    \n",
                "    return {\n",
                "        \"embeddings\": embeddings,\n",
                "        \"provider\": reflex_llms.get_selected_provider(),\n",
                "        \"count\": len(embeddings),\n",
                "        \"dimension\": len(embeddings[0]) if embeddings else 0\n",
                "    }\n",
                "\n",
                "# Example usage\n",
                "sample_texts = [\n",
                "    \"OpenAI provides state-of-the-art language models\",\n",
                "    \"RefLex enables seamless fallback to local AI servers\",\n",
                "    \"GPT models excel at natural language understanding\",\n",
                "    \"Local AI deployment reduces dependency on cloud services\"\n",
                "]\n",
                "\n",
                "try:\n",
                "    result = get_openai_embeddings(sample_texts)\n",
                "    print(f\"Generated {result['count']} embeddings using {result['provider']}\")\n",
                "    print(f\"Embedding dimension: {result['dimension']}\")\n",
                "    \n",
                "    # Calculate similarity between first two texts\n",
                "    if len(result['embeddings']) >= 2:\n",
                "        import math\n",
                "        \n",
                "        def cosine_similarity(a, b):\n",
                "            dot_product = sum(x * y for x, y in zip(a, b))\n",
                "            magnitude_a = math.sqrt(sum(x * x for x in a))\n",
                "            magnitude_b = math.sqrt(sum(x * x for x in b))\n",
                "            return dot_product / (magnitude_a * magnitude_b)\n",
                "        \n",
                "        similarity = cosine_similarity(result['embeddings'][0], result['embeddings'][1])\n",
                "        print(f\"Similarity between first two texts: {similarity:.3f}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Embedding generation failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Streaming Responses Example"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "\n",
                "def stream_openai_response(prompt=\"Tell me about artificial intelligence\"):\n",
                "    \"\"\"Demonstrate streaming responses from OpenAI with fallback.\"\"\"\n",
                "    client = reflex_llms.get_openai_client(from_file=\"reflex_openai.json\")\n",
                "    \n",
                "    print(f\"Streaming response from {reflex_llms.get_selected_provider()}...\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    try:\n",
                "        stream = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "            max_tokens=200,\n",
                "            stream=True\n",
                "        )\n",
                "        \n",
                "        full_response = \"\"\n",
                "        for chunk in stream:\n",
                "            if chunk.choices[0].delta.content is not None:\n",
                "                content = chunk.choices[0].delta.content\n",
                "                print(content, end=\"\", flush=True)\n",
                "                full_response += content\n",
                "        \n",
                "        print(\"\\n\" + \"=\" * 50)\n",
                "        print(f\"Stream complete. Total characters: {len(full_response)}\")\n",
                "        \n",
                "        return full_response\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Streaming failed: {e}\")\n",
                "        print(\"Falling back to regular completion...\")\n",
                "        \n",
                "        # Fallback to non-streaming\n",
                "        response = client.chat.completions.create(\n",
                "            model=\"gpt-3.5-turbo\",\n",
                "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "            max_tokens=200\n",
                "        )\n",
                "        \n",
                "        fallback_response = response.choices[0].message.content\n",
                "        print(fallback_response)\n",
                "        return fallback_response\n",
                "\n",
                "# Test streaming\n",
                "streaming_result = stream_openai_response(\n",
                "    \"Explain the advantages of using OpenAI models in production applications\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## OpenAI Production Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import reflex_llms\n",
                "from reflex_llms.server import ReflexServerConfig, ModelMapping\n",
                "\n",
                "class OpenAIManager:\n",
                "    \"\"\"Production-ready OpenAI manager with fallback.\"\"\"\n",
                "    \n",
                "    def __init__(self, environment=\"production\"):\n",
                "        self.environment = environment\n",
                "        self.client = None\n",
                "        self._setup_openai_client()\n",
                "    \n",
                "    def _setup_openai_client(self):\n",
                "        \"\"\"Setup OpenAI client with production configuration.\"\"\"\n",
                "        \n",
                "        if self.environment == \"development\":\n",
                "            # Development: prefer local for cost savings\n",
                "            fallback_config = ReflexServerConfig(\n",
                "                port=11434,\n",
                "                container_name=\"dev-openai-fallback\",\n",
                "                model_mappings=ModelMapping(\n",
                "                    minimal_setup=True,\n",
                "                    minimal_model_mapping={\n",
                "                        \"gpt-3.5-turbo\": \"llama3.2:3b\",\n",
                "                        \"gpt-4o-mini\": \"gemma3:2b\"\n",
                "                    }\n",
                "                )\n",
                "            )\n",
                "            preference = [\"reflex\", \"openai\"]\n",
                "            \n",
                "        else:\n",
                "            # Production: prefer OpenAI for quality\n",
                "            fallback_config = ReflexServerConfig(\n",
                "                host=\"0.0.0.0\",\n",
                "                port=8080,\n",
                "                container_name=\"prod-openai-fallback\",\n",
                "                model_mappings=ModelMapping(\n",
                "                    minimal_setup=False,\n",
                "                    model_mapping={\n",
                "                        \"gpt-3.5-turbo\": \"llama3.2:7b\",\n",
                "                        \"gpt-4\": \"llama3.1:70b\",\n",
                "                        \"gpt-4o\": \"llama3.1:405b\",\n",
                "                        \"gpt-4o-mini\": \"gemma3:27b\",\n",
                "                        \"text-embedding-ada-002\": \"nomic-embed-text\"\n",
                "                    }\n",
                "                )\n",
                "            )\n",
                "            preference = [\"openai\", \"reflex\"]\n",
                "        \n",
                "        self.client = reflex_llms.get_openai_client(\n",
                "            preference_order=preference,\n",
                "            timeout=30.0 if self.environment == \"production\" else 10.0,\n",
                "            reflex_server_config=fallback_config\n",
                "        )\n",
                "        \n",
                "        provider = reflex_llms.get_selected_provider()\n",
                "        print(f\"OpenAI Manager ({self.environment}) ready using: {provider}\")\n",
                "        \n",
                "        if provider == \"reflex\" and self.environment == \"production\":\n",
                "            print(\"⚠️  Production using local fallback - check OpenAI connectivity\")\n",
                "    \n",
                "    def chat_completion(self, messages, model=\"gpt-3.5-turbo\", **kwargs):\n",
                "        \"\"\"OpenAI chat completion with automatic fallback.\"\"\"\n",
                "        try:\n",
                "            return self.client.chat.completions.create(\n",
                "                model=model,\n",
                "                messages=messages,\n",
                "                **kwargs\n",
                "            )\n",
                "        except Exception as e:\n",
                "            print(f\"OpenAI request failed: {e}\")\n",
                "            # Attempt fallback\n",
                "            reflex_llms.clear_cache()\n",
                "            self._setup_openai_client()\n",
                "            \n",
                "            return self.client.chat.completions.create(\n",
                "                model=model,\n",
                "                messages=messages,\n",
                "                **kwargs\n",
                "            )\n",
                "    \n",
                "    def get_embeddings(self, text, model=\"text-embedding-ada-002\"):\n",
                "        \"\"\"OpenAI embeddings with fallback.\"\"\"\n",
                "        return self.client.embeddings.create(\n",
                "            model=model,\n",
                "            input=text\n",
                "        )\n",
                "    \n",
                "    def stream_completion(self, messages, model=\"gpt-3.5-turbo\", **kwargs):\n",
                "        \"\"\"Streaming completion for real-time responses.\"\"\"\n",
                "        return self.client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            stream=True,\n",
                "            **kwargs\n",
                "        )\n",
                "    \n",
                "    def health_check(self):\n",
                "        \"\"\"Check OpenAI system health.\"\"\"\n",
                "        status = reflex_llms.get_module_status()\n",
                "        \n",
                "        health = {\n",
                "            \"provider\": status[\"selected_provider\"],\n",
                "            \"openai_primary\": status[\"selected_provider\"] == \"openai\",\n",
                "            \"fallback_available\": status.get(\"reflex_server_running\", False),\n",
                "            \"config_cached\": status[\"has_cached_config\"],\n",
                "            \"environment\": self.environment\n",
                "        }\n",
                "        \n",
                "        return health\n",
                "    \n",
                "    def cost_estimate(self, tokens_input, tokens_output, model=\"gpt-3.5-turbo\"):\n",
                "        \"\"\"Estimate OpenAI API costs (rough estimates).\"\"\"\n",
                "        # Rough pricing as of 2024 (check OpenAI pricing for current rates)\n",
                "        pricing = {\n",
                "            \"gpt-3.5-turbo\": {\"input\": 0.0015 / 1000, \"output\": 0.002 / 1000},\n",
                "            \"gpt-4\": {\"input\": 0.03 / 1000, \"output\": 0.06 / 1000},\n",
                "            \"gpt-4o\": {\"input\": 0.005 / 1000, \"output\": 0.015 / 1000},\n",
                "            \"gpt-4o-mini\": {\"input\": 0.00015 / 1000, \"output\": 0.0006 / 1000}\n",
                "        }\n",
                "        \n",
                "        if model in pricing:\n",
                "            cost = (tokens_input * pricing[model][\"input\"] + \n",
                "                   tokens_output * pricing[model][\"output\"])\n",
                "            return {\"estimated_cost\": cost, \"model\": model, \"note\": \"Rough estimate\"}\n",
                "        else:\n",
                "            return {\"error\": \"Model pricing not available\"}\n",
                "    \n",
                "    def cleanup(self):\n",
                "        \"\"\"Clean shutdown.\"\"\"\n",
                "        if reflex_llms.is_using_reflex():\n",
                "            reflex_llms.stop_reflex_server()\n",
                "            print(\"Fallback server stopped\")\n",
                "\n",
                "# Usage examples\n",
                "print(\"Setting up Development Environment:\")\n",
                "dev_ai = OpenAIManager(\"development\")\n",
                "\n",
                "print(\"\\nSetting up Production Environment:\")\n",
                "prod_ai = OpenAIManager(\"production\")\n",
                "\n",
                "# Test different models\n",
                "try:\n",
                "    # Development test\n",
                "    dev_response = dev_ai.chat_completion(\n",
                "        messages=[{\"role\": \"user\", \"content\": \"Hello from development!\"}],\n",
                "        model=\"gpt-3.5-turbo\",\n",
                "        max_tokens=30\n",
                "    )\n",
                "    print(f\"\\nDev response: {dev_response.choices[0].message.content}\")\n",
                "    \n",
                "    # Production test\n",
                "    prod_response = prod_ai.chat_completion(\n",
                "        messages=[{\"role\": \"user\", \"content\": \"Hello from production!\"}],\n",
                "        model=\"gpt-4o-mini\",\n",
                "        max_tokens=30\n",
                "    )\n",
                "    print(f\"Prod response: {prod_response.choices[0].message.content}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Test failed: {e}\")\n",
                "\n",
                "# Health checks\n",
                "dev_health = dev_ai.health_check()\n",
                "prod_health = prod_ai.health_check()\n",
                "\n",
                "print(f\"\\nDevelopment Health: {dev_health}\")\n",
                "print(f\"Production Health: {prod_health}\")\n",
                "\n",
                "# Cost estimation example\n",
                "cost_estimate = prod_ai.cost_estimate(100, 50, \"gpt-4o-mini\")\n",
                "print(f\"\\nCost estimate: ${cost_estimate.get('estimated_cost', 0):.6f}\")\n",
                "\n",
                "# Cleanup\n",
                "dev_ai.cleanup()\n",
                "prod_ai.cleanup()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## OpenAI Monitoring and Troubleshooting"
            ]
        }
    ]
}