{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "aedae277",
            "metadata": {},
            "source": [
                "# RefLex LLM - Complete OpenAI Integration Guide\n",
                "\n",
                "RefLex LLM is an intelligent OpenAI API fallback system that automatically switches between OpenAI, Azure OpenAI, and local AI when endpoints become unavailable. It provides seamless failover capabilities while maintaining full OpenAI API compatibility. The primary intent is to use the module for testing and CI run, as local execution might be slower but also less expensive. In the future, with the possibility of spinning up a load balanced reflex kubernetes cluster, reflex could be shaped into a failsafe mechanism.\n",
                "\n",
                "## What is RefLex LLM?\n",
                "\n",
                "RefLex LLM acts as an intelligent middleware layer between your application and various AI providers. When your primary OpenAI endpoint fails due to rate limits, outages, or network issues, RefLex automatically detects the failure and routes your requests to alternative providers without any code changes required.\n",
                "\n",
                "## Key Features\n",
                "\n",
                "- **Automatic Provider Selection**:  \n",
                "Intelligently chooses between OpenAI, Azure OpenAI, and local Ollama based on availability and your preferences\n",
                "- **Docker Integration**:  \n",
                "Automatically manages local AI containers with zero configuration\n",
                "- **OpenAI Compatibility**:  \n",
                "Drop-in replacement for the OpenAI Python client with identical API\n",
                "- **Model Mapping**:  \n",
                "Automatically maps OpenAI model names to equivalent local models\n",
                "- **Configuration Management**:  \n",
                "Supports file-based configuration for different environments\n",
                "- **Health Monitoring**:  \n",
                "Continuous health checking and automatic recovery\n",
                "- **Performance Optimization**:  \n",
                "Caches configurations and maintains persistent connections\n",
                "\n",
                "## Installation and Setup\n",
                "\n",
                "RefLex requires Python 3.8+ and Docker for local AI capabilities. The installation includes all necessary dependencies including the OpenAI client, Docker SDK, and configuration management tools."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d67f6cfe",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install reflex-llms numpy"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "972e64cb",
            "metadata": {},
            "source": [
                "## Provider Resolution and Basic Usage\n",
                "\n",
                "RefLex automatically detects which AI providers are available and selects the best option based on your preference order. The system performs intelligent health checks by testing each provider in sequence and uses the first one that responds successfully.\n",
                "\n",
                "### How Provider Testing Works\n",
                "\n",
                "The provider resolution process involves several sophisticated steps:\n",
                "\n",
                "1. **OpenAI Testing**: Makes test HTTP requests to api.openai.com or your custom endpoint, checking for valid API responses (200 or 401 status codes indicate a working endpoint)\n",
                "\n",
                "2. **Azure Testing**: If Azure credentials are configured, tests the Azure OpenAI endpoint for accessibility and valid authentication\n",
                "\n",
                "3. **RefLex Local**: Automatically starts Docker containers if needed, manages Ollama installation, and verifies local model availability\n",
                "\n",
                "4. **Caching**: Successful configurations are cached to avoid repeated health checks and improve performance\n",
                "\n",
                "The system is designed to be resilient and will automatically retry failed providers and handle network timeouts gracefully."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e5afa4b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from reflex_llms import (\n",
                "    get_openai_client, \n",
                "    get_selected_provider,\n",
                "    get_module_status,\n",
                "    is_using_reflex\n",
                ")\n",
                "\n",
                "# Configure client with provider preferences\n",
                "client = get_openai_client(\n",
                "    preference_order=[\"openai\", \"reflex\"],\n",
                "    openai_base_url=\"https://wrong.address.com/v1\",  # Force fallback for demo\n",
                "    timeout=5.0\n",
                ")\n",
                "\n",
                "# Display system status\n",
                "status = get_module_status()\n",
                "print(f\"Selected provider: {get_selected_provider()}\")\n",
                "print(f\"Using local RefLex: {is_using_reflex()}\")\n",
                "print(f\"Config cached: {status['has_cached_config']}\")\n",
                "print(f\"RefLex server running: {status['reflex_server_running']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a5494ebd",
            "metadata": {},
            "source": [
                "## Chat Completions with Automatic Failover\n",
                "\n",
                "RefLex provides identical OpenAI API functionality regardless of the underlying provider. All standard OpenAI parameters work seamlessly, including temperature, max_tokens, system messages, and advanced features like function calling. The client automatically handles provider differences behind the scenes, ensuring your application code remains unchanged.\n",
                "\n",
                "### Response Handling and Metadata\n",
                "\n",
                "When you make requests through RefLex, you receive standard OpenAI response objects with additional metadata about which provider was used. This transparency allows you to monitor provider usage patterns and optimize your configuration accordingly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de758e28",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import display utilities\n",
                "from utils import display_message, display_stream, display_embeddings\n",
                "\n",
                "# Standard chat completion\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Explain how RefLex LLM works in simple terms.\"}],\n",
                "    max_tokens=150\n",
                ")\n",
                "\n",
                "# Display formatted response\n",
                "display_message(response, as_markdown=True)\n",
                "\n",
                "print(f\"Model used: {response.model}\")\n",
                "print(f\"Tokens: {response.usage.total_tokens if response.usage else 'Unknown'}\")\n",
                "print(f\"Provider: {get_selected_provider()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ee7e404c",
            "metadata": {},
            "source": [
                "## Model Management and Automatic Mapping\n",
                "\n",
                "One of RefLex's most powerful features is its intelligent model mapping system. When using the RefLex provider, requests for OpenAI models like \"gpt-3.5-turbo\" are automatically routed to compatible local models such as \"llama3.2:3b\". This mapping is configurable and can be customized based on your specific needs.\n",
                "\n",
                "### Model Categories and Organization\n",
                "\n",
                "RefLex organizes available models into logical categories to help you understand what's available and choose the right model for your task. The system automatically handles model downloads, updates, and lifecycle management."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b9238152",
            "metadata": {},
            "outputs": [],
            "source": [
                "# List and categorize available models\n",
                "models = client.models.list()\n",
                "\n",
                "chat_models = []\n",
                "embedding_models = []\n",
                "reasoning_models = []\n",
                "\n",
                "for model in models.data:\n",
                "    model_id = model.id\n",
                "    if \"embedding\" in model_id:\n",
                "        embedding_models.append(model_id)\n",
                "    elif any(x in model_id for x in [\"o1\", \"o3\", \"o4\", \"reasoning\"]):\n",
                "        reasoning_models.append(model_id)\n",
                "    elif any(x in model_id for x in [\"gpt\", \"llama\", \"gemma\"]):\n",
                "        chat_models.append(model_id)\n",
                "\n",
                "print(f\"Available models ({len(models.data)} total):\")\n",
                "print(f\"Chat models: {len(chat_models)}\")\n",
                "print(f\"Reasoning models: {len(reasoning_models)}\")\n",
                "print(f\"Embedding models: {len(embedding_models)}\")\n",
                "\n",
                "# Show sample models\n",
                "print(f\"\\nSample chat models: {sorted(chat_models)[:3]}\")\n",
                "print(f\"Sample reasoning models: {sorted(reasoning_models)[:3]}\")\n",
                "print(f\"Sample embedding models: {sorted(embedding_models)[:3]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0b7fdcf7",
            "metadata": {},
            "source": [
                "## Working with Embeddings\n",
                "\n",
                "RefLex fully supports OpenAI's embeddings API through local models, providing significant cost savings for applications that process large amounts of text. The text-embedding models are automatically mapped to compatible local alternatives like nomic-embed-text, which often provide comparable quality to OpenAI's models.\n",
                "\n",
                "### Embedding Quality and Performance\n",
                "\n",
                "Local embedding models can process text without network latency and API rate limits, making them ideal for batch processing, real-time applications, and privacy-sensitive use cases where data cannot leave your infrastructure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "72e9ae69",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create embeddings\n",
                "embedding_response = client.embeddings.create(\n",
                "    model=\"text-embedding-ada-002\",\n",
                "    input=\"RefLex LLM provides seamless fallback between OpenAI and local AI models.\"\n",
                ")\n",
                "\n",
                "# Display embedding information using utility function\n",
                "display_embeddings(embedding_response, show_stats=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c7c590f4",
            "metadata": {},
            "source": [
                "## Real-time Streaming Responses\n",
                "\n",
                "RefLex fully supports OpenAI's streaming API, enabling real-time response generation that's essential for interactive applications like chatbots, coding assistants, and live content generation. The streaming functionality works identically across all providers, ensuring consistent user experience regardless of which backend is serving the request.\n",
                "\n",
                "### Streaming Benefits and Use Cases\n",
                "\n",
                "Streaming is particularly valuable for longer responses where users can start reading while the response is still being generated. This significantly improves perceived performance and user engagement, especially important for applications with real-time chat interfaces or interactive content generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50759faf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create streaming request\n",
                "stream = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"Write a brief explanation of AI failover systems.\"}],\n",
                "    max_tokens=200,\n",
                "    stream=True,\n",
                "    temperature=0.7\n",
                ")\n",
                "\n",
                "# Display streaming response\n",
                "full_response = display_stream(stream)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2db4cb39",
            "metadata": {},
            "source": [
                "## Advanced Reasoning Models\n",
                "\n",
                "RefLex provides access to specialized reasoning models through the o1, o3, and o4 series. These models are specifically designed for complex problem-solving, mathematical reasoning, and step-by-step analytical tasks. They excel at breaking down complex problems, showing their work, and providing detailed explanations of their reasoning process.\n",
                "\n",
                "### When to Use Reasoning Models\n",
                "\n",
                "Reasoning models are particularly effective for mathematical problems, logical puzzles, code debugging, system design questions, and any task that benefits from explicit step-by-step thinking. They typically use lower temperature settings for more focused and consistent reasoning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1ca01762",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test reasoning model with markdown streaming\n",
                "reasoning_stream = client.chat.completions.create(\n",
                "    model=\"o4-mini\",\n",
                "    messages=[{\"role\": \"user\", \"content\": \"A company has 3 servers handling 100 users each. Load increases 50% monthly. How many servers needed after 6 months?\"}],\n",
                "    max_tokens=300,\n",
                "    stream=True,\n",
                "    temperature=0.1\n",
                ")\n",
                "\n",
                "# Stream with markdown formatting\n",
                "reasoning_response = display_stream(reasoning_stream, as_markdown=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ce285d47",
            "metadata": {},
            "source": [
                "## Server Management and Infrastructure Control\n",
                "\n",
                "When using the RefLex provider, you gain access to powerful server management capabilities that allow you to monitor, control, and configure your local AI infrastructure. This includes container lifecycle management, model deployment, health monitoring, and resource optimization.\n",
                "\n",
                "### Server Access and Monitoring\n",
                "\n",
                "The server management interface provides real-time visibility into your local AI infrastructure, including container status, model availability, resource usage, and performance metrics. This transparency is crucial for production deployments where you need to ensure reliable service availability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2447c0f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from reflex_llms import get_reflex_server\n",
                "\n",
                "# Access RefLex server instance\n",
                "server = get_reflex_server()\n",
                "\n",
                "if server:\n",
                "    print(f\"API URL: {server.api_url}\")\n",
                "    print(f\"OpenAI Compatible URL: {server.openai_compatible_url}\")\n",
                "    print(f\"Host: {server.host}\")\n",
                "    print(f\"Port: {server.port}\")\n",
                "    print(f\"Container: {server.container_name}\")\n",
                "    print(f\"Running: {server.is_running}\")\n",
                "    print(f\"Healthy: {server.is_healthy}\")\n",
                "    \n",
                "    # Detailed status\n",
                "    status = server.get_status()\n",
                "    print(f\"Setup complete: {status.get('setup_complete', False)}\")\n",
                "    print(f\"Total models: {status.get('total_models', 0)}\")\n",
                "    print(f\"OpenAI models: {len(status.get('openai_compatible_models', []))}\")\n",
                "    \n",
                "else:\n",
                "    print(f\"Not using RefLex server. Current provider: {get_selected_provider()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1173cc11",
            "metadata": {},
            "source": [
                "## Configuration Management and Customization\n",
                "\n",
                "RefLex supports sophisticated configuration management through JSON files that allow you to customize every aspect of the system's behavior. This includes provider preferences, timeout settings, model mappings, server configurations, and environment-specific overrides.\n",
                "\n",
                "### Configuration Structure and Hierarchy\n",
                "\n",
                "The configuration system follows a clear hierarchy where function parameters override environment variables, which override configuration file settings. This flexibility allows you to maintain base configurations in files while providing runtime overrides for specific deployments.\n",
                "\n",
                "Configuration files support:\n",
                "- Provider preference orders for different environments\n",
                "- Custom API endpoints and authentication settings\n",
                "- RefLex server container and deployment configurations\n",
                "- Model mapping customizations for specific use cases\n",
                "- Performance tuning parameters like timeouts and retry logic\n",
                "- Environment-specific overrides for development, staging, and production"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "62ff8f22",
            "metadata": {},
            "outputs": [],
            "source": [
                "from utils import display_json\n",
                "display_json(\"reflex.json\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8c9e49ef",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load client from file\n",
                "client = get_openai_client(\n",
                "    preference_order=[\"openai\", \"reflex\"],\n",
                "    openai_base_url=\"https://wrong.address.com/v1\",\n",
                "    from_file=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7e6b7488",
            "metadata": {},
            "source": [
                "## Performance Optimization and Production Deployment\n",
                "\n",
                "RefLex is designed for production use with comprehensive performance optimization features and enterprise-grade reliability. The system includes intelligent caching, connection pooling, health monitoring, and automatic recovery mechanisms.\n",
                "\n",
                "### Performance Optimization Strategies\n",
                "\n",
                "- **Provider Selection Optimization**: Place fastest and most reliable providers first in your preference order to minimize latency\n",
                "- **Configuration Caching**: Configurations are automatically cached to avoid repeated health checks and improve response times\n",
                "- **Model Selection Strategy**: Choose appropriate models for specific tasks - smaller models for simple tasks, specialized models for complex reasoning\n",
                "- **Local Server Optimization**: Use minimal_setup=True for faster startup times during development\n",
                "- **Timeout Configuration**: Set appropriate timeouts based on your application's latency requirements\n",
                "\n",
                "### Production Deployment Considerations\n",
                "\n",
                "- **Security Best Practices**: Store API keys securely in environment variables, use HTTPS for all communications, implement proper authentication\n",
                "- **Reliability and High Availability**: Configure multiple fallback providers, implement health checks and alerting, use persistent storage for model data\n",
                "- **Monitoring and Observability**: Track provider selection patterns, monitor response times and error rates, implement comprehensive logging\n",
                "- **Scaling and Resource Management**: Use container orchestration for multiple instances, implement horizontal pod autoscaling, plan for model storage requirements"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd0c54c7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance optimization example\n",
                "optimized_config = {\n",
                "    \"preference_order\": [\"openai\", \"reflex\"],  # Skip unused providers\n",
                "    \"timeout\": 3.0,  # Faster timeout\n",
                "    \"reflex_server\": {\n",
                "        \"model_mappings\": {\n",
                "            \"minimal_setup\": True,  # Faster startup\n",
                "            \"minimal_model_mapping\": {\n",
                "                \"gpt-3.5-turbo\": \"llama3.2:1b\",  # Smaller, faster model\n",
                "                \"gpt-4o-mini\": \"llama3.2:1b\"\n",
                "            }\n",
                "        }\n",
                "    }\n",
                "}\n",
                "\n",
                "import json\n",
                "print(\"Optimized Configuration:\")\n",
                "print(json.dumps(optimized_config, indent=2))\n",
                "\n",
                "# Environment-based configuration\n",
                "import os\n",
                "environment = os.getenv('ENVIRONMENT', 'development')\n",
                "\n",
                "if environment == 'development':\n",
                "    preference = [\"reflex\", \"openai\"]\n",
                "elif environment == 'production':\n",
                "    preference = [\"openai\", \"azure\"]\n",
                "else:\n",
                "    preference = [\"reflex\"]\n",
                "\n",
                "print(f\"Environment: {environment}\")\n",
                "print(f\"Provider preference: {preference}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "496ed317",
            "metadata": {},
            "source": [
                "## Error Handling and System Diagnostics\n",
                "\n",
                "RefLex provides robust error handling and recovery mechanisms designed to handle real-world deployment scenarios. The system gracefully handles common issues like Docker unavailability, network connectivity problems, model unavailability, and port conflicts.\n",
                "\n",
                "### Common Scenarios and Automatic Recovery\n",
                "\n",
                "- **Docker Not Running**: System gracefully falls back to cloud providers without interruption\n",
                "- **Network Connectivity Issues**: Automatic retries with configurable timeouts and exponential backoff\n",
                "- **Model Unavailability**: Intelligent model mapping with automatic downloading and version management\n",
                "- **Port Conflicts**: Automatic port management, container cleanup, and conflict resolution\n",
                "- **Provider Rate Limiting**: Automatic failover to alternative providers when limits are reached\n",
                "\n",
                "### System Diagnostics and Troubleshooting\n",
                "\n",
                "The diagnostic system provides comprehensive visibility into system state, configuration status, and provider health. This information is essential for troubleshooting issues and optimizing performance in production environments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "71a45781",
            "metadata": {},
            "outputs": [],
            "source": [
                "from reflex_llms import clear_cache, stop_reflex_server\n",
                "\n",
                "# System diagnostics\n",
                "final_status = get_module_status()\n",
                "print(f\"Provider: {final_status['selected_provider']}\")\n",
                "print(f\"RefLex available: {final_status['reflex_server_running']}\")\n",
                "print(f\"Configuration cached: {final_status['has_cached_config']}\")\n",
                "\n",
                "# Environment checks\n",
                "import os\n",
                "print(f\"OpenAI API Key: {'Set' if os.getenv('OPENAI_API_KEY') else 'Not Set'}\")\n",
                "print(f\"Azure endpoint: {'Set' if os.getenv('AZURE_OPENAI_ENDPOINT') else 'Not Set'}\")\n",
                "\n",
                "# Cache management\n",
                "print(f\"Use clear_cache() to force provider re-resolution\")\n",
                "print(f\"Use stop_reflex_server() to clean up local resources\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "464c3f6b",
            "metadata": {},
            "source": [
                "## Summary and Best Practices\n",
                "\n",
                "RefLex LLM provides a comprehensive solution for building reliable AI-powered applications with automatic failover capabilities. The system is designed to handle the complexities of multi-provider AI deployment while maintaining the simplicity of the standard OpenAI API.\n",
                "\n",
                "### Core Advantages and Value Proposition\n",
                "\n",
                "- **Reliability and High Availability**: Automatic failover ensures applications remain operational even during provider outages or rate limiting\n",
                "- **Cost Efficiency**: Local models significantly reduce API costs for development, testing, and batch processing workloads\n",
                "- **API Compatibility**: Perfect drop-in replacement for OpenAI Python client with zero code changes required\n",
                "- **Operational Flexibility**: Support for multiple providers and deployment configurations from development to enterprise scale\n",
                "- **Developer Experience**: Minimal configuration required with intelligent defaults and comprehensive documentation\n",
                "\n",
                "### Implementation Best Practices\n",
                "\n",
                "- **Configuration Management**: Use configuration files for environment-specific settings and maintain clear separation between development, staging, and production configurations\n",
                "- **Testing and Validation**: Regularly test failover scenarios in staging environments to ensure smooth operation during actual outages\n",
                "- **Monitoring and Observability**: Implement comprehensive monitoring of provider selection patterns, response times, and error rates\n",
                "- **Performance Optimization**: Monitor and optimize provider preferences, model selections, and timeout configurations based on actual usage patterns\n",
                "- **Security and Compliance**: Follow security best practices for credential management, network access control, and data handling\n",
                "\n",
                "RefLex LLM seamlessly bridges cloud and local AI infrastructure, ensuring applications remain operational and cost-effective across diverse deployment scenarios. The automatic failover and intelligent model mapping capabilities make it an ideal foundation for production AI applications that require both high availability and operational flexibility."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
